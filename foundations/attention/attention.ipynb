{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/the-attention-mechanism-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention\n",
      "[[0.98522025 1.74174051 0.75652026]\n",
      " [0.90965265 1.40965265 0.5       ]\n",
      " [0.99851226 1.75849334 0.75998108]\n",
      " [0.99560386 1.90407309 0.90846923]]\n",
      "weights\n",
      "[[2.36089863e-01 7.38987555e-03 7.49130386e-01 7.38987555e-03]\n",
      " [4.54826323e-01 4.51736775e-02 4.54826323e-01 4.51736775e-02]\n",
      " [2.39275049e-01 7.43870015e-04 7.59237211e-01 7.43870015e-04]\n",
      " [8.99501754e-02 2.81554063e-03 9.05653685e-01 1.58059922e-03]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import random\n",
    "from numpy import dot\n",
    "from scipy.special import softmax\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# encoder representations of four different words\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])\n",
    "\n",
    "# stacking the word embeddings into a single array\n",
    "words = array(\n",
    "    [\n",
    "        word_1,\n",
    "        word_2,\n",
    "        word_3,\n",
    "        word_4,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# generating the weight matrices\n",
    "W_Q = random.randint(3, size=(3, 3))\n",
    "W_K = random.randint(3, size=(3, 3))\n",
    "W_V = random.randint(3, size=(3, 3))\n",
    "\n",
    "# generating the queries, keys and values\n",
    "Q = words @ W_Q\n",
    "K = words @ W_K\n",
    "V = words @ W_V\n",
    "\n",
    "# scoring the query vectors against all key vectors\n",
    "scores = Q @ K.transpose()\n",
    "\n",
    "# computing the weights by a softmax operation\n",
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
    "\n",
    "# computing the attention by a weighted sum of the value vectors\n",
    "attention = weights @ V\n",
    "\n",
    "print('attention')\n",
    "print(attention)\n",
    "print('weights')\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  2, 10,  2],\n",
       "       [ 4,  0,  4,  0],\n",
       "       [12,  2, 14,  2],\n",
       "       [10,  4, 14,  3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0],\n",
       "       [0, 1, 1, 0],\n",
       "       [1, 1, 2, 0],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words @ words.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  2, 10,  2],\n",
       "       [ 4,  0,  4,  0],\n",
       "       [12,  2, 14,  2],\n",
       "       [10,  4, 14,  3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words @ W_Q @ W_K.T @ words.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sooftware/attentions/blob/master/attentions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    https://paperswithcode.com/method/scaled\n",
    "    Scaled Dot-Product Attention proposed in \"Attention Is All You Need\"\n",
    "    Compute the dot products of the query with all keys, divide each by sqrt(dim),\n",
    "    and apply a softmax function to obtain the weights on the values\n",
    "\n",
    "    Args: dim, mask\n",
    "        dim (int): dimention of attention\n",
    "        mask (torch.Tensor): tensor containing indices to be masked\n",
    "\n",
    "    Inputs: query, key, value, mask\n",
    "        - **query** (batch, q_len, d_model): tensor containing projection vector for decoder.\n",
    "        - **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n",
    "        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
    "        - **mask** (-): tensor containing indices to be masked\n",
    "\n",
    "    Returns: context, attn\n",
    "        - **context**: tensor containing the context vector from attention mechanism.\n",
    "        - **attn**: tensor containing the attention (alignment) from the encoder outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.sqrt_dim = np.sqrt(dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Tensor,\n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        mask: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n",
    "\n",
    "        if mask is not None:\n",
    "            score.masked_fill_(mask.view(score.size()), -float(\"Inf\"))\n",
    "\n",
    "        attn = F.softmax(score, -1)\n",
    "        context = torch.bmm(attn, value)\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 3]), torch.Size([1, 3, 4]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(10, 3).unsqueeze(dim=0)\n",
    "mat2 = torch.randn(3, 4).unsqueeze(dim=0)\n",
    "# res = torch.bmm(input, mat2)\n",
    "input.shape, mat2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.9944, 1.7971, 0.8027],\n",
       "          [0.9442, 1.4442, 0.5000],\n",
       "          [0.9997, 1.8040, 0.8043],\n",
       "          [0.9988, 1.9427, 0.9439]]]),\n",
       " tensor([[[1.9448e-01, 2.7946e-03, 7.9993e-01, 2.7946e-03],\n",
       "          [4.7210e-01, 2.7904e-02, 4.7210e-01, 2.7904e-02],\n",
       "          [1.9551e-01, 1.6605e-04, 8.0416e-01, 1.6605e-04],\n",
       "          [5.5740e-02, 8.0097e-04, 9.4306e-01, 3.9493e-04]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_attn = ScaledDotProductAttention(dim=2)\n",
    "scaled_dot_attn.forward(\n",
    "    query=torch.tensor(Q, dtype=torch.float32).unsqueeze(dim=0),\n",
    "    key=torch.tensor(K, dtype=torch.float32).unsqueeze(dim=0),\n",
    "    value=torch.tensor(V, dtype=torch.float32).unsqueeze(dim=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute the dot products of the query with all values and apply a softmax function to obtain the weights on the values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query: Tensor, value: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        batch_size, hidden_dim, input_size = query.size(0), query.size(2), value.size(1)\n",
    "        print(batch_size, hidden_dim, input_size)\n",
    "        score = torch.bmm(query, value.transpose(1, 2))\n",
    "        attn = F.softmax(score.view(-1, input_size), dim=1).view(\n",
    "            batch_size, -1, input_size\n",
    "        )\n",
    "        context = torch.bmm(attn, value)\n",
    "\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8808, 1.7616, 0.8808],\n",
       "          [0.8808, 1.3808, 0.5000],\n",
       "          [0.9820, 1.8628, 0.8808],\n",
       "          [0.9526, 1.9051, 0.9526]]]),\n",
       " tensor([[[0.1050, 0.1050, 0.7758, 0.0142],\n",
       "          [0.4404, 0.0596, 0.4404, 0.0596],\n",
       "          [0.1171, 0.0158, 0.8650, 0.0021],\n",
       "          [0.0452, 0.0452, 0.9074, 0.0022]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_attn = DotProductAttention()\n",
    "dot_attn.forward(\n",
    "    query=torch.tensor(Q, dtype=torch.float32).unsqueeze(dim=0),\n",
    "    value=torch.tensor(V, dtype=torch.float32).unsqueeze(dim=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"attention\" only on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# hidden states -> torch.Size([2, 1024, 1024])\n",
    "# hidden states -> torch.Size([2, 1024, 1024])\n",
    "\n",
    "a = torch.randn([2, 1024, 1024])\n",
    "b = torch.randn([2, 1024, 1024])\n",
    "scaled_dot_attn = ScaledDotProductAttention(dim=2)\n",
    "scaled_dot_attn.forward(\n",
    "    query=a,\n",
    "    key=a,\n",
    "    value=a,\n",
    ")[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.scaled_dot_product_attention(\n",
    "\tquery=a,\n",
    "\tkey=a,\n",
    "\tvalue=a,\n",
    ").shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"markdown","metadata":{},"source":["https://www.kaggle.com/code/suraj520/srgan-psnr-19db-10-epoch-know-train-infer"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:37:41.460072Z","iopub.status.busy":"2023-07-18T20:37:41.459139Z","iopub.status.idle":"2023-07-18T20:37:44.204621Z","shell.execute_reply":"2023-07-18T20:37:44.203550Z","shell.execute_reply.started":"2023-07-18T20:37:41.460018Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim as optim\n","from torchvision.models.vgg import vgg16\n","from math import exp\n","import torch\n","import torch.nn.functional as F\n","import torchvision.utils as utils\n","from torch.autograd import Variable\n","from tqdm import tqdm\n","import math\n","import pandas as pd\n","import os\n","from os import listdir\n","import numpy as np\n","from PIL import Image\n","from os.path import join"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:37:44.207542Z","iopub.status.busy":"2023-07-18T20:37:44.206880Z","iopub.status.idle":"2023-07-18T20:37:44.273083Z","shell.execute_reply":"2023-07-18T20:37:44.271882Z","shell.execute_reply.started":"2023-07-18T20:37:44.207500Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","upscale_factor = 8\n","crop_size = 88\n","num_epochs = 10\n","\n","mean = np.array([0.485, 0.456, 0.406])\n","std = np.array([0.229, 0.224, 0.225])\n","\n","\n","def is_image(filename):\n","    return any(\n","        filename.endswith(extension)\n","        for extension in [\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\"]\n","    )\n","\n","\n","def calc_valid_crop_size(crop_size, upscale_factor):\n","    return crop_size - (crop_size % upscale_factor)\n","\n","\n","def train_high_res_transform(crop_size):\n","    return transforms.Compose([transforms.RandomCrop(crop_size), transforms.ToTensor()])\n","\n","\n","def train_low_res_transform(crop_size, upscale_factor):\n","    return transforms.Compose(\n","        [\n","            transforms.ToPILImage(),\n","            transforms.Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n","            transforms.ToTensor(),\n","        ]\n","    )"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:37:44.330107Z","iopub.status.busy":"2023-07-18T20:37:44.328467Z","iopub.status.idle":"2023-07-18T20:37:44.343143Z","shell.execute_reply":"2023-07-18T20:37:44.341994Z","shell.execute_reply.started":"2023-07-18T20:37:44.330073Z"},"trusted":true},"outputs":[],"source":["class TrainDataFromFolder(Dataset):\n","    def __init__(self, data_dir, crop_size, upscale_factor):\n","        super().__init__()\n","        self.image_file_names = [\n","            join(data_dir, x) for x in listdir(data_dir) if is_image(x)\n","        ]\n","        crop_size = calc_valid_crop_size(crop_size, upscale_factor)\n","        self.high_res_transform = train_high_res_transform(crop_size)\n","        self.low_res_transform = train_low_res_transform(crop_size, upscale_factor)\n","\n","    def __getitem__(self, index):\n","        hr_image = self.high_res_transform(Image.open(self.image_file_names[index]))\n","        lr_image = self.low_res_transform(hr_image)\n","        return lr_image, hr_image\n","\n","    def __len__(self):\n","        return len(self.image_file_names)\n","\n","\n","class ValDataFromFolder(Dataset):\n","    def __init__(self, data_dir, upscale_factor):\n","        super().__init__()\n","        self.upscale_factor = upscale_factor\n","        self.image_file_names = [\n","            join(data_dir, x) for x in listdir(data_dir) if is_image(x)\n","        ]\n","\n","    def __getitem__(self, index):\n","        hr_image = Image.open(self.image_file_names[index])\n","        w, h = hr_image.size\n","        crop_size = calc_valid_crop_size(min(w, h), self.upscale_factor)\n","        lr_scale = transforms.Resize(\n","            crop_size // self.upscale_factor, interpolation=Image.BICUBIC\n","        )\n","        hr_scale = transforms.Resize(crop_size, interpolation=Image.BICUBIC)\n","        hr_image = transforms.CenterCrop(crop_size)(hr_image)\n","        lr_image = lr_scale(hr_image)\n","        hr_restored_image = hr_scale(lr_image)\n","        return (\n","            transforms.ToTensor()(lr_image),\n","            transforms.ToTensor()(hr_restored_image),\n","            transforms.ToTensor()(hr_image),\n","        )\n","\n","    def __len__(self):\n","        return len(self.image_file_names)\n","\n","\n","train_set = TrainDataFromFolder(\n","    \"ebany_research/weight_superresolution/dataset/train/\",\n","    crop_size=crop_size,\n","    upscale_factor=upscale_factor,\n",")\n","val_set = ValDataFromFolder(\n","    \"ebany_research/weight_superresolution/dataset/valid\", upscale_factor=upscale_factor\n",")\n","train_loader = DataLoader(dataset=train_set, num_workers=0, batch_size=1, shuffle=True)\n","val_loader = DataLoader(dataset=val_set, num_workers=0, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 3, 11, 11])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["next(iter(train_loader))[0].shape"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-output":true,"collapsed":true,"execution":{"iopub.execute_input":"2023-07-18T20:37:44.698955Z","iopub.status.busy":"2023-07-18T20:37:44.698222Z","iopub.status.idle":"2023-07-18T20:38:27.833025Z","shell.execute_reply":"2023-07-18T20:38:27.831345Z","shell.execute_reply.started":"2023-07-18T20:37:44.698898Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[tensor([[[[0.2275, 0.2235, 0.2235, 0.2196, 0.2157, 0.2196, 0.2196, 0.2196,\n","           0.2157, 0.2157, 0.2157],\n","          [0.2275, 0.2275, 0.2235, 0.2235, 0.2235, 0.2196, 0.2196, 0.2235,\n","           0.2235, 0.2235, 0.2196],\n","          [0.2314, 0.2353, 0.2314, 0.2314, 0.2275, 0.2235, 0.2235, 0.2275,\n","           0.2275, 0.2275, 0.2235],\n","          [0.2392, 0.2392, 0.2353, 0.2353, 0.2314, 0.2314, 0.2314, 0.2314,\n","           0.2275, 0.2275, 0.2235],\n","          [0.2431, 0.2392, 0.2392, 0.2353, 0.2353, 0.2353, 0.2353, 0.2353,\n","           0.2314, 0.2275, 0.2275],\n","          [0.2431, 0.2431, 0.2392, 0.2392, 0.2392, 0.2353, 0.2353, 0.2353,\n","           0.2353, 0.2314, 0.2314],\n","          [0.2471, 0.2471, 0.2471, 0.2392, 0.2392, 0.2392, 0.2392, 0.2353,\n","           0.2353, 0.2353, 0.2353],\n","          [0.2471, 0.2471, 0.2510, 0.2431, 0.2431, 0.2431, 0.2392, 0.2392,\n","           0.2392, 0.2392, 0.2353],\n","          [0.2510, 0.2510, 0.2510, 0.2471, 0.2431, 0.2431, 0.2431, 0.2392,\n","           0.2392, 0.2392, 0.2353],\n","          [0.2510, 0.2510, 0.2510, 0.2471, 0.2471, 0.2471, 0.2431, 0.2431,\n","           0.2431, 0.2431, 0.2392],\n","          [0.2510, 0.2510, 0.2471, 0.2471, 0.2471, 0.2471, 0.2471, 0.2471,\n","           0.2471, 0.2431, 0.2431]],\n","\n","         [[0.2784, 0.2784, 0.2745, 0.2784, 0.2745, 0.2745, 0.2745, 0.2745,\n","           0.2706, 0.2667, 0.2667],\n","          [0.2784, 0.2784, 0.2784, 0.2824, 0.2784, 0.2784, 0.2745, 0.2784,\n","           0.2784, 0.2745, 0.2667],\n","          [0.2863, 0.2863, 0.2902, 0.2863, 0.2863, 0.2824, 0.2824, 0.2824,\n","           0.2824, 0.2784, 0.2745],\n","          [0.2902, 0.2941, 0.2902, 0.2902, 0.2863, 0.2863, 0.2863, 0.2863,\n","           0.2863, 0.2824, 0.2784],\n","          [0.2980, 0.2980, 0.2941, 0.2941, 0.2941, 0.2902, 0.2902, 0.2902,\n","           0.2863, 0.2863, 0.2824],\n","          [0.2980, 0.2980, 0.2980, 0.2941, 0.2980, 0.2941, 0.2941, 0.2941,\n","           0.2902, 0.2902, 0.2863],\n","          [0.3059, 0.3059, 0.3020, 0.2980, 0.2980, 0.2980, 0.2980, 0.2980,\n","           0.2902, 0.2902, 0.2902],\n","          [0.3098, 0.3059, 0.3059, 0.3020, 0.3020, 0.3020, 0.2980, 0.2980,\n","           0.2941, 0.2941, 0.2941],\n","          [0.3098, 0.3098, 0.3098, 0.3059, 0.3059, 0.3059, 0.3020, 0.3020,\n","           0.2980, 0.2980, 0.2980],\n","          [0.3176, 0.3137, 0.3137, 0.3098, 0.3098, 0.3098, 0.3059, 0.3020,\n","           0.3020, 0.3020, 0.2980],\n","          [0.3176, 0.3176, 0.3137, 0.3137, 0.3137, 0.3137, 0.3098, 0.3059,\n","           0.3059, 0.3020, 0.3020]],\n","\n","         [[0.4078, 0.4039, 0.4039, 0.4078, 0.4039, 0.4039, 0.3961, 0.3922,\n","           0.3882, 0.3882, 0.3882],\n","          [0.4118, 0.4078, 0.4078, 0.4078, 0.4078, 0.4078, 0.4039, 0.4000,\n","           0.3961, 0.3922, 0.3882],\n","          [0.4196, 0.4157, 0.4196, 0.4157, 0.4157, 0.4118, 0.4118, 0.4078,\n","           0.4039, 0.4039, 0.3961],\n","          [0.4235, 0.4235, 0.4235, 0.4196, 0.4196, 0.4157, 0.4157, 0.4118,\n","           0.4118, 0.4118, 0.4078],\n","          [0.4314, 0.4314, 0.4314, 0.4235, 0.4235, 0.4235, 0.4235, 0.4196,\n","           0.4157, 0.4157, 0.4157],\n","          [0.4392, 0.4353, 0.4353, 0.4353, 0.4314, 0.4275, 0.4275, 0.4235,\n","           0.4235, 0.4235, 0.4196],\n","          [0.4471, 0.4431, 0.4392, 0.4353, 0.4353, 0.4353, 0.4314, 0.4314,\n","           0.4275, 0.4314, 0.4235],\n","          [0.4510, 0.4471, 0.4471, 0.4431, 0.4431, 0.4431, 0.4392, 0.4353,\n","           0.4353, 0.4353, 0.4275],\n","          [0.4588, 0.4549, 0.4510, 0.4510, 0.4471, 0.4471, 0.4471, 0.4431,\n","           0.4431, 0.4392, 0.4353],\n","          [0.4667, 0.4588, 0.4588, 0.4549, 0.4549, 0.4549, 0.4510, 0.4510,\n","           0.4471, 0.4431, 0.4431],\n","          [0.4824, 0.4784, 0.4706, 0.4667, 0.4627, 0.4627, 0.4588, 0.4549,\n","           0.4549, 0.4510, 0.4510]]]]), tensor([[[[0.2235, 0.2078, 0.2314,  ..., 0.2275, 0.2235, 0.2235],\n","          [0.2235, 0.2235, 0.2196,  ..., 0.2118, 0.2196, 0.2314],\n","          [0.2353, 0.2196, 0.2078,  ..., 0.2157, 0.2118, 0.2157],\n","          ...,\n","          [0.2431, 0.2471, 0.2471,  ..., 0.2353, 0.2431, 0.2431],\n","          [0.2471, 0.2510, 0.2510,  ..., 0.2431, 0.2431, 0.2471],\n","          [0.2510, 0.2471, 0.2549,  ..., 0.2431, 0.2471, 0.2471]],\n","\n","         [[0.2745, 0.2588, 0.2824,  ..., 0.2706, 0.2667, 0.2667],\n","          [0.2784, 0.2745, 0.2706,  ..., 0.2627, 0.2667, 0.2745],\n","          [0.2784, 0.2706, 0.2588,  ..., 0.2667, 0.2667, 0.2667],\n","          ...,\n","          [0.3098, 0.3137, 0.3137,  ..., 0.2902, 0.3020, 0.3020],\n","          [0.3216, 0.3216, 0.3216,  ..., 0.3059, 0.3020, 0.3059],\n","          [0.3216, 0.3216, 0.3216,  ..., 0.3020, 0.3020, 0.3059]],\n","\n","         [[0.4039, 0.3882, 0.4157,  ..., 0.3961, 0.3882, 0.3882],\n","          [0.4039, 0.4039, 0.3961,  ..., 0.3804, 0.3922, 0.4000],\n","          [0.4078, 0.4000, 0.3922,  ..., 0.3922, 0.3882, 0.3882],\n","          ...,\n","          [0.4667, 0.4745, 0.4784,  ..., 0.4157, 0.4588, 0.4431],\n","          [0.4824, 0.4824, 0.4902,  ..., 0.4471, 0.4431, 0.4588],\n","          [0.4941, 0.4941, 0.4941,  ..., 0.4471, 0.4588, 0.4667]]]])]\n"]}],"source":["for batch in train_loader:\n","    print(batch)\n","    break"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:38:27.852821Z","iopub.status.busy":"2023-07-18T20:38:27.851887Z","iopub.status.idle":"2023-07-18T20:38:27.864422Z","shell.execute_reply":"2023-07-18T20:38:27.863292Z","shell.execute_reply.started":"2023-07-18T20:38:27.852780Z"},"trusted":true},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.prelu = nn.PReLU()\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        residual = self.conv1(x)\n","        residual = self.bn1(residual)\n","        residual = self.prelu(residual)\n","        residual = self.conv2(residual)\n","        residual = self.bn2(residual)\n","        return x + residual\n","\n","\n","class UpsampleBlock(nn.Module):\n","    def __init__(self, in_channels, up_scale):\n","        super(UpsampleBlock, self).__init__()\n","        self.conv = nn.Conv2d(\n","            in_channels, in_channels * up_scale**2, kernel_size=3, padding=1\n","        )\n","        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n","        self.prelu = nn.PReLU()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.pixel_shuffle(x)\n","        x = self.prelu(x)\n","        return x\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, scale_factor):\n","        super(Generator, self).__init__()\n","        upsample_block_num = int(math.log(scale_factor, 2))\n","\n","        self.block1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=9, padding=4), nn.PReLU()\n","        )\n","\n","        self.block2 = ResidualBlock(64)\n","        self.block3 = ResidualBlock(64)\n","        self.block4 = ResidualBlock(64)\n","        self.block5 = ResidualBlock(64)\n","        self.block6 = ResidualBlock(64)\n","        self.block7 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64)\n","        )\n","        block8 = [UpsampleBlock(64, 2) for _ in range(upsample_block_num)]\n","        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n","        self.block8 = nn.Sequential(*block8)\n","\n","    def forward(self, x):\n","        block1 = self.block1(x)\n","        block2 = self.block2(block1)\n","        block3 = self.block3(block2)\n","        block4 = self.block4(block3)\n","        block5 = self.block5(block4)\n","        block6 = self.block6(block5)\n","        block7 = self.block7(block6)\n","        block8 = self.block8(block1 + block7)\n","        return (torch.tanh(block8) + 1) / 2\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2),\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(512, 1024, kernel_size=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(1024, 1, kernel_size=1),\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.size()[0]\n","        return torch.sigmoid(self.net(x).view(batch_size))\n","\n","\n","class TVLoss(nn.Module):\n","    def __init__(self, tv_loss_weight=1):\n","        super(TVLoss, self).__init__()\n","        self.tv_loss_weight = tv_loss_weight\n","\n","    def forward(self, x):\n","        batch_size = x.size()[0]\n","        h_x = x.size()[2]\n","        w_x = x.size()[3]\n","\n","        count_h = self.tensor_size(x[:, :, 1:, :])\n","        count_w = self.tensor_size(x[:, :, :, 1:])\n","\n","        h_tv = torch.pow(x[:, :, 1:, :] - x[:, :, : h_x - 1, :], 2).sum()\n","        w_tv = torch.pow(x[:, :, :, 1:] - x[:, :, :, : w_x - 1], 2).sum()\n","        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n","\n","    @staticmethod\n","    def tensor_size(t):\n","        return t.size()[1] * t.size()[2] * t.size()[3]\n","\n","\n","class GeneratorLoss(nn.Module):\n","    def __init__(self):\n","        super(GeneratorLoss, self).__init__()\n","        vgg = vgg16(pretrained=True)\n","        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n","        for param in loss_network.parameters():\n","            param.requires_grad = False\n","        self.loss_network = loss_network\n","        self.mse_loss = nn.MSELoss()\n","        self.tv_loss = TVLoss()\n","\n","    def forward(self, out_labels, out_images, target_images):\n","        adversial_loss = torch.mean(1 - out_labels)\n","        perception_loss = self.mse_loss(out_images, target_images)\n","        image_loss = self.mse_loss(out_images, target_images)\n","        tv_loss = self.tv_loss(out_images)\n","        return (\n","            image_loss\n","            + 0.001 * adversial_loss\n","            + 0.006 * perception_loss\n","            + 2e-8 * tv_loss\n","        )"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:38:27.935374Z","iopub.status.busy":"2023-07-18T20:38:27.934949Z","iopub.status.idle":"2023-07-18T20:38:28.020191Z","shell.execute_reply":"2023-07-18T20:38:28.019169Z","shell.execute_reply.started":"2023-07-18T20:38:27.935337Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["netG = Generator(upscale_factor)\n","netD = Discriminator()\n","\n","generator_criterion = GeneratorLoss()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:38:33.171723Z","iopub.status.busy":"2023-07-18T20:38:33.171246Z","iopub.status.idle":"2023-07-18T20:38:35.896576Z","shell.execute_reply":"2023-07-18T20:38:35.895423Z","shell.execute_reply.started":"2023-07-18T20:38:33.171683Z"},"trusted":true},"outputs":[],"source":["generator_criterion = generator_criterion.to(device)\n","netG = netG.to(device)\n","netD = netD.to(device)\n","\n","optimizerG = optim.Adam(netG.parameters(), lr=0.0002)\n","optimizerD = optim.Adam(netD.parameters(), lr=0.0002)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:38:35.909898Z","iopub.status.busy":"2023-07-18T20:38:35.908774Z","iopub.status.idle":"2023-07-18T20:38:35.929474Z","shell.execute_reply":"2023-07-18T20:38:35.928311Z","shell.execute_reply.started":"2023-07-18T20:38:35.909855Z"},"trusted":true},"outputs":[],"source":["def gaussian(window_size, sigma):\n","    gauss = torch.Tensor(\n","        [\n","            exp(-((x - window_size // 2) ** 2) / float(2 * sigma**2))\n","            for x in range(window_size)\n","        ]\n","    )\n","    return gauss / gauss.sum()\n","\n","\n","def create_window(window_size, channel):\n","    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n","    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n","    window = Variable(\n","        _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n","    )\n","    return window\n","\n","\n","def _ssim(img1, img2, window, window_size, channel, size_average=True):\n","    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n","    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n","\n","    mu1_sq = mu1.pow(2)\n","    mu2_sq = mu2.pow(2)\n","    mu1_mu2 = mu1 * mu2\n","\n","    sigma1_sq = (\n","        F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n","    )\n","    sigma2_sq = (\n","        F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n","    )\n","    sigma12 = (\n","        F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel)\n","        - mu1_mu2\n","    )\n","\n","    C1 = 0.01**2\n","    C2 = 0.03**2\n","\n","    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / (\n","        (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n","    )\n","\n","    if size_average:\n","        return ssim_map.mean()\n","    else:\n","        return ssim_map.mean(1).mean(1).mean(1)\n","\n","\n","def ssim(img1, img2, window_size=11, size_average=True):\n","    (_, channel, _, _) = img1.size()\n","    window = create_window(window_size, channel)\n","\n","    if img1.is_cuda:\n","        window = window.cuda(img1.get_device())\n","    window = window.type_as(img1)\n","\n","    return _ssim(img1, img2, window, window_size, channel, size_average)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:38:35.931733Z","iopub.status.busy":"2023-07-18T20:38:35.931181Z","iopub.status.idle":"2023-07-18T20:38:35.944824Z","shell.execute_reply":"2023-07-18T20:38:35.943719Z","shell.execute_reply.started":"2023-07-18T20:38:35.931694Z"},"trusted":true},"outputs":[],"source":["results = {\n","    \"d_loss\": [],\n","    \"g_loss\": [],\n","    \"d_score\": [],\n","    \"g_score\": [],\n","    \"psnr\": [],\n","    \"ssim\": [],\n","}"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \n","📌\n","    <b>Remarks</b>    <br>\n","\n","1. Mean square loss is often not great since it compares pixel values, SSIM(Structural similarity) tries to capture the structure of image including noise via statistic. SSIM looks at groups of pixels to decipher whether two images are same or not.<br>\n","2. psnr is peak signal to noise ratio used as yet another metric.<br>\n","3. TV loss obtains better edges by doing total variation(TV) on the reconstructed image and the residual between the reconstructed image and the original image.<br>\n","4. Training model simultaneously.</div>"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:38:35.947173Z","iopub.status.busy":"2023-07-18T20:38:35.946548Z","iopub.status.idle":"2023-07-18T20:53:39.952339Z","shell.execute_reply":"2023-07-18T20:53:39.949248Z","shell.execute_reply.started":"2023-07-18T20:38:35.947130Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[1/10] Loss_D: 0.9908 Loss_G: 0.1120 D(x): 0.4977 D(G(z)): 0.4147: 100%|██████████| 2/2 [00:00<00:00,  2.16it/s]\n","  0%|          | 0/1 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["[converting LR images to SR images] PSNR: 11.7755 dB SSIM: 0.3275: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n","[2/10] Loss_D: 0.9765 Loss_G: 0.1022 D(x): 0.3906 D(G(z)): 0.3251: 100%|██████████| 2/2 [00:00<00:00,  7.49it/s]\n","[converting LR images to SR images] PSNR: 11.2465 dB SSIM: 0.3147: 100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n","[3/10] Loss_D: 0.9654 Loss_G: 0.0355 D(x): 0.3457 D(G(z)): 0.2895: 100%|██████████| 2/2 [00:00<00:00,  7.95it/s]\n","[converting LR images to SR images] PSNR: 10.6299 dB SSIM: 0.3012: 100%|██████████| 1/1 [00:00<00:00,  6.03it/s]\n","[4/10] Loss_D: 0.9565 Loss_G: 0.0451 D(x): 0.3301 D(G(z)): 0.2794: 100%|██████████| 2/2 [00:00<00:00,  7.92it/s]\n","[converting LR images to SR images] PSNR: 9.6403 dB SSIM: 0.2786: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n","[5/10] Loss_D: 0.9103 Loss_G: 0.0717 D(x): 0.3762 D(G(z)): 0.2781: 100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n","[converting LR images to SR images] PSNR: 9.1134 dB SSIM: 0.2662: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n","[6/10] Loss_D: 0.8914 Loss_G: 0.0742 D(x): 0.3953 D(G(z)): 0.2863: 100%|██████████| 2/2 [00:00<00:00,  7.98it/s]\n","[converting LR images to SR images] PSNR: 9.7224 dB SSIM: 0.2802: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n","[7/10] Loss_D: 0.8802 Loss_G: 0.0399 D(x): 0.4167 D(G(z)): 0.3000: 100%|██████████| 2/2 [00:00<00:00,  7.52it/s]\n","[converting LR images to SR images] PSNR: 9.8877 dB SSIM: 0.2745: 100%|██████████| 1/1 [00:00<00:00,  6.14it/s]\n","[8/10] Loss_D: 0.8176 Loss_G: 0.0243 D(x): 0.4888 D(G(z)): 0.3121: 100%|██████████| 2/2 [00:00<00:00,  7.56it/s]\n","[converting LR images to SR images] PSNR: 9.5059 dB SSIM: 0.2496: 100%|██████████| 1/1 [00:00<00:00,  6.03it/s]\n","[9/10] Loss_D: 0.7636 Loss_G: 0.0294 D(x): 0.5597 D(G(z)): 0.3263: 100%|██████████| 2/2 [00:00<00:00,  7.42it/s]\n","[converting LR images to SR images] PSNR: 8.6727 dB SSIM: 0.2115: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n","[10/10] Loss_D: 0.6957 Loss_G: 0.0274 D(x): 0.6336 D(G(z)): 0.3189: 100%|██████████| 2/2 [00:00<00:00,  7.44it/s]\n","[converting LR images to SR images] PSNR: 8.0784 dB SSIM: 0.1856: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\n"]}],"source":["for epoch in range(1, num_epochs + 1):\n","    train_bar = tqdm(train_loader)\n","    running_results = {\n","        \"batch_sizes\": 0,\n","        \"d_loss\": 0,\n","        \"g_loss\": 0,\n","        \"d_score\": 0,\n","        \"g_score\": 0,\n","    }\n","\n","    netG.train()\n","    netD.train()\n","    for data, target in train_bar:\n","        g_update_first = True\n","        batch_size = data.size(0)\n","        running_results[\"batch_sizes\"] += batch_size\n","\n","        ############################\n","        # (1) Update D network: maximize D(x)-1-D(G(z))\n","        ###########################\n","        real_img = Variable(target)\n","        if torch.cuda.is_available():\n","            real_img = real_img.cuda()\n","        z = Variable(data)\n","        if torch.cuda.is_available():\n","            z = z.cuda()\n","        fake_img = netG(z)\n","\n","        netD.zero_grad()\n","        real_out = netD(real_img).mean()\n","        fake_out = netD(fake_img).mean()\n","        d_loss = 1 - real_out + fake_out\n","        d_loss.backward(retain_graph=True)\n","        optimizerD.step()\n","\n","        ############################\n","        # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n","        ###########################\n","        netG.zero_grad()\n","        ## The two lines below are added to prevent runetime error in Google Colab ##\n","        fake_img = netG(z)\n","        fake_out = netD(fake_img).mean()\n","        ##\n","        g_loss = generator_criterion(fake_out, fake_img, real_img)\n","        g_loss.backward()\n","\n","        fake_img = netG(z)\n","        fake_out = netD(fake_img).mean()\n","\n","        optimizerG.step()\n","\n","        # loss for current batch before optimization\n","        running_results[\"g_loss\"] += g_loss.item() * batch_size\n","        running_results[\"d_loss\"] += d_loss.item() * batch_size\n","        running_results[\"d_score\"] += real_out.item() * batch_size\n","        running_results[\"g_score\"] += fake_out.item() * batch_size\n","\n","        train_bar.set_description(\n","            desc=\"[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f\"\n","            % (\n","                epoch,\n","                num_epochs,\n","                running_results[\"d_loss\"] / running_results[\"batch_sizes\"],\n","                running_results[\"g_loss\"] / running_results[\"batch_sizes\"],\n","                running_results[\"d_score\"] / running_results[\"batch_sizes\"],\n","                running_results[\"g_score\"] / running_results[\"batch_sizes\"],\n","            )\n","        )\n","\n","    netG.eval()\n","\n","    with torch.no_grad():\n","        val_bar = tqdm(val_loader)\n","        valid_results = {\"mse\": 0, \"ssims\": 0, \"psnr\": 0, \"ssim\": 0, \"batch_sizes\": 0}\n","        val_images = []\n","        for val_lr, val_hr_restore, val_hr in val_bar:\n","            batch_size = val_lr.size(0)\n","            valid_results[\"batch_sizes\"] += batch_size\n","            lr = val_lr\n","            hr = val_hr\n","            if torch.cuda.is_available():\n","                lr = lr.cuda()\n","                hr = hr.cuda()\n","            sr = netG(lr)\n","\n","            batch_mse = ((sr - hr) ** 2).data.mean()\n","            valid_results[\"mse\"] += batch_mse * batch_size\n","            batch_ssim = ssim(sr, hr).item()\n","            valid_results[\"ssims\"] += batch_ssim * batch_size\n","            valid_results[\"psnr\"] = 10 * math.log10(\n","                (hr.max() ** 2) / (valid_results[\"mse\"] / valid_results[\"batch_sizes\"])\n","            )\n","            valid_results[\"ssim\"] = (\n","                valid_results[\"ssims\"] / valid_results[\"batch_sizes\"]\n","            )\n","            val_bar.set_description(\n","                desc=\"[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f\"\n","                % (valid_results[\"psnr\"], valid_results[\"ssim\"])\n","            )\n","\n","    if not os.path.exists(\"epochs/\"):\n","        os.makedirs(\"epochs/\")\n","    # save model parameters\n","    if epoch % 10 == 0:\n","        torch.save(\n","            netG.state_dict(),\n","            \"ebany_research/weight_superresolution/epochs/netG_epoch_%d_%d.pth\"\n","            % (upscale_factor, epoch),\n","        )\n","        torch.save(\n","            netD.state_dict(),\n","            \"ebany_research/weight_superresolution/epochs/netD_epoch_%d_%d.pth\"\n","            % (upscale_factor, epoch),\n","        )\n","    # save loss\\scores\\psnr\\ssim\n","    results[\"d_loss\"].append(running_results[\"d_loss\"] / running_results[\"batch_sizes\"])\n","    results[\"g_loss\"].append(running_results[\"g_loss\"] / running_results[\"batch_sizes\"])\n","    results[\"d_score\"].append(\n","        running_results[\"d_score\"] / running_results[\"batch_sizes\"]\n","    )\n","    results[\"g_score\"].append(\n","        running_results[\"g_score\"] / running_results[\"batch_sizes\"]\n","    )\n","    results[\"psnr\"].append(valid_results[\"psnr\"])\n","    results[\"ssim\"].append(valid_results[\"ssim\"])\n","\n","    # if epoch % 10 == 0 and epoch != 0:\n","    #     out_path = 'statistics/'\n","    #     if not os.path.exists(out_path):\n","    #       os.makedirs(out_path)\n","\n","    #     data_frame = pd.DataFrame(\n","    #         data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n","    #               'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n","    #         index=range(1, epoch + 1))\n","    #     data_frame.to_csv(out_path + 'srf_' + str(upscale_factor) + '_train_results.csv', index_label='Epoch')"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \n","📌\n","Evaluating trained model on test image    </div>\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:55:30.497220Z","iopub.status.busy":"2023-07-18T20:55:30.495966Z","iopub.status.idle":"2023-07-18T20:55:30.561042Z","shell.execute_reply":"2023-07-18T20:55:30.559983Z","shell.execute_reply.started":"2023-07-18T20:55:30.497168Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["upscale_factor = 8\n","model_name = \"netG_epoch_8_20.pth\"\n","model = Generator(upscale_factor).eval()\n","device = torch.device(\"cuda\")\n","model = model.to(device)\n","model.load_state_dict(torch.load(\"ebany_research/weight_superresolution/epochs/netG_epoch_8_10.pth\"))"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \n","📌\n","saving the output and displaying it    </div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T20:55:31.850697Z","iopub.status.busy":"2023-07-18T20:55:31.850121Z"},"trusted":true},"outputs":[],"source":["# pass any other image, if needed\n","image_name = \"ebany_research/weight_superresolution/dataset/valid/0003.png\"\n","image = Image.open(image_name)\n","image = Variable(transforms.ToTensor()(image)).unsqueeze(0).to(device)\n","out = model(image)\n","out_img = transforms.ToPILImage()(out[0].data.cpu())\n","out_img.save(\"output.jpeg\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 3, 1356, 2040])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["image.shape"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \n","📌\n","CUDA memory error   </div>\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \n","📌\n","Verifying the input dimensions to observe the superresolution using SRGAN  </div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(image.shape)\n","print(out_img.shape)"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \n","📌\n","Displaying the input and output images  </div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(image)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(out_img)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

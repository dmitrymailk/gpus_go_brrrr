{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7241748480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/user-name-goes-here/.local/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from ebany_research.llm_lora.changed_mistral import MistralForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "device = 3\n",
    "model = MistralForCausalLM.from_pretrained(model_name, device_map={\"\": device})\n",
    "model = model.eval()\n",
    "# 29410MiB\n",
    "model = model.half()\n",
    "print(count_parameters(model))\n",
    "# model = MistralForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "# )\n",
    "# model = MistralForCausalLM._from_config(config=config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "dataset = load_dataset(\"openaccess-ai-collective/oo-gpt4-filtered\")\n",
    "dataset = dataset[\"train\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'system_prompt', 'question', 'response', '__index_level_0__'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 32001,  1587,    13,  1976,   460,   396, 16107, 13892, 28723,\n",
       "           995,   622,   347,  2078,   264,  3638, 28723,   995,  1580,  8270,\n",
       "           264, 10537,   304,  1043,  4372, 28723, 32000, 32001,  2188,    13,\n",
       "            13,    13, 28798,   556,   276,  1987, 11285, 28742, 28713,  3487,\n",
       "         20415,   298,  1912,   706,   684,   272,  6400,   354,   272,  8982,\n",
       "          4150, 28723, 28705, 12628,   272,  2996,   345, 11273,   400,  1987,\n",
       "           767,   863, 14151,   927,   298,   511, 28804,   548,   349,   345,\n",
       "         10847,   264,  4150, 28739,   264,  3716,  4372, 28804,    13,  2820,\n",
       "         16981, 28747, 32000, 32001, 13892,    13,  2501, 28725,   345, 10847,\n",
       "           264,  4150, 28739,   349,   459,   264,  3716,  4372,   298,   272,\n",
       "          2996,   345, 11273,   400,  1987,   767,   863, 14151,   927,   298,\n",
       "           511,  1110, 28723,   415,  2996, 21165,   356,   767, 14151,  3236,\n",
       "           298,   511,  1159,  2492,   272,  4126,  1034,   298, 11285, 28742,\n",
       "         28713,  3487, 20415, 28725,  3210,   821,   272,  4695,  1759,   302,\n",
       "          2170,  3864,   304, 15832,   272,  8982,  4150,  3837, 28723,   330,\n",
       "           680,  7658,  4372,   682, 17516,   272,  9796,   304,  1917,   697,\n",
       "         14151,   553,   298, 16353,   621,  4681,   298,  3754,   288, 11285,\n",
       "         28742, 28713,  3487, 20415,   684,   272,  8982,  4150,  6400, 28723,\n",
       "            13,    13, 11600,  2572,  3716, 11194,   298,   272,  2996,   829,\n",
       "          3024, 28747,    13,    13, 28740, 28723,  9116,   272,  8982,  4150,\n",
       "         28747,  7337,  6852, 11285, 28742, 28713,  3487, 20415, 28725, 14151,\n",
       "           682,   927,   298,   506,   264, 12230,  3028,   302,   767,   272,\n",
       "          8982,  4150,   682,   936,   614, 28723,   851,   829, 17516, 24568,\n",
       "           356,   264,  3608, 28725,   727, 28725,   304,  4723, 28725,   390,\n",
       "          1162,   390, 23689,   272, 11683,  1274, 28723,    13,    13, 28750,\n",
       "         28723, 23184,  4296,   395,  2663, 28747, 14151,  1659,   506,  3236,\n",
       "           298,  7731,   395,  2848,  3282,   304,  2005,  3338,   298,  1316,\n",
       "           395,  4150,  7394, 28725, 20083,   369,   272,  8982,  4150,   682,\n",
       "          8753,   395,  3376, 28742, 28713, 22245,  2464,   304, 22731, 28723,\n",
       "            13,    13, 28770, 28723,  5158, 21824,  4150,  4162, 28747,  7337,\n",
       "          6852, 11285, 28742, 28713,  3487, 20415, 28725, 14151,   682,   927,\n",
       "           298,  1917,  4118, 10936,   302,   272,  4150, 28725,  1259,   390,\n",
       "           272,  7335, 28725,  8059,   697, 28725,  2887,   304, 16195, 28725,\n",
       "          6290,   442, 15175, 28725,   304,   707,  8326, 14841, 28723,    13,\n",
       "            13, 28781, 28723,  8648,   442, 10221,  1304, 25964, 28747,  1791,\n",
       "         25491, 19411, 11285, 28742, 28713,  3487, 20415, 28725, 14151,   993,\n",
       "           506,  3236,   298,  2231,  5277,   442,  7153,  1304, 25964,   369,\n",
       "          4658,   544,  8598,  4162,   684,   272,  8982,  4150, 28723,    13,\n",
       "            13, 28782, 28723,  8702,   653,   264,  2796,  2838, 28747,   560,\n",
       "          1745,   298,  9087,   272,  2442,   302,  8982, 28725, 14151,   682,\n",
       "           927,   298,  1847,   864,   264,  2623,   442,  2796,  2838,   369,\n",
       "           682, 18951, 11285,   298,  8986,   272,  1951,  1671, 24593,   871,\n",
       "          1132,  6032, 28723,    13,    13,  2675,   455, 28725,   272,  2996,\n",
       "         21165,   356,   272,  7892,  5377,  5944,  3030,   354, 14151,   298,\n",
       "         10130,  1034, 11285, 28742, 28713,  3487, 20415,   304,  5227,   706,\n",
       "           684,   272,  8982,  4150,  6400, 28723,  8469, 28725,   345, 10847,\n",
       "           264,  4150, 28739,  1235,   459, 24329,  2962,   272,  6768, 14151,\n",
       "          3236,   298,  1388,  1159,  2492,   272,  4126,  1034, 28723, 32000,\n",
       "         32001, 13892,    13],\n",
       "        [32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "         32000,     1, 32001,  1587,    13,  1976,   460,   396, 16107, 13892,\n",
       "         28723,  1247,   622,   368,  2111,   368,   264,  3638, 28723,  3604,\n",
       "          5541,   349,   298,  4160,   272,  3638,   390,  7152,  3071,   390,\n",
       "           368,   541, 28723,  4023, 13801,   272,  3638,  1073,  3707, 28733,\n",
       "          1403, 28733,  7691,   304, 16205,   574,  5944, 28723, 32000, 32001,\n",
       "          2188,    13,  5238,   264, 12271,   297, 10177, 28723, 32000, 32001,\n",
       "         13892,    13,  9977, 28705, 28740, 28747, 21815,   264,  3817,   354,\n",
       "           272, 12271, 28723,    13, 28737,   622,  4987,   345,  5773,   660,\n",
       "           311, 28739,   325,  1237,  3914, 28731,   390,   272,  3817, 28723,\n",
       "            13,    13,  9977, 28705, 28750, 28747, 13341,   264, 12143,   304,\n",
       "           264, 28445, 28723,    13, 28737,   622,   938,   272, 12143,   345,\n",
       "          4754, 17749, 28739,   325,   532,  1482, 28731,   297,   272,  2169,\n",
       "         28445, 28723,    13,    13,  9977, 28705, 28770, 28747,  1325, 28768,\n",
       "           786,   380,   272, 12143, 28723,    13, 24091,   345,  5773,   660,\n",
       "           311, 28739,   349,   264, 15816, 28725,  4008, 28733,  9701,  3817,\n",
       "         28725,   315,   622, 11365,   786,   380,   345,  4754, 17749, 28739,\n",
       "           297,   272,  2169, 28445,   390,   345,  4754,   267,   611,    13,\n",
       "            13,  9977, 28705, 28781, 28747,  4849,   272, 12271, 28723,    13,\n",
       "          8479, 28725,   486, 27698,   272,  3817,   304,   272, 11365,   786,\n",
       "           601, 12143, 28725,   272, 12271,   297, 10177,   349, 28747,   345,\n",
       "          5773,   660,   311,  7952,   611,    13,    13, 10202,  2500, 28747,\n",
       "           851, 12271,   349,  3588, 28725,   847,  6461,  6789,  4714, 28725,\n",
       "           304,  6695,  1002,   272,  4979, 11365,   786,   352,   302,   272,\n",
       "         12143,   345,  4754, 17749, 28739,   297,   272,  2169, 28445,   354,\n",
       "           272,  3817,   345,  5773,   660,   311,   611,   661,  7254,  1002,\n",
       "           298,   345,  1014,  3914,  7825, 28739,   297,  4300, 28723, 32000,\n",
       "         32001, 13892,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[    1, 32001,  1587,    13,  1976,   460,   396, 16107, 13892, 28723,\n",
       "           995,   622,   347,  2078,   264,  3638, 28723,   995,  1580,  8270,\n",
       "           264, 10537,   304,  1043,  4372, 28723,  -100, 32001,  2188,    13,\n",
       "            13,    13, 28798,   556,   276,  1987, 11285, 28742, 28713,  3487,\n",
       "         20415,   298,  1912,   706,   684,   272,  6400,   354,   272,  8982,\n",
       "          4150, 28723, 28705, 12628,   272,  2996,   345, 11273,   400,  1987,\n",
       "           767,   863, 14151,   927,   298,   511, 28804,   548,   349,   345,\n",
       "         10847,   264,  4150, 28739,   264,  3716,  4372, 28804,    13,  2820,\n",
       "         16981, 28747,  -100, 32001, 13892,    13,  2501, 28725,   345, 10847,\n",
       "           264,  4150, 28739,   349,   459,   264,  3716,  4372,   298,   272,\n",
       "          2996,   345, 11273,   400,  1987,   767,   863, 14151,   927,   298,\n",
       "           511,  1110, 28723,   415,  2996, 21165,   356,   767, 14151,  3236,\n",
       "           298,   511,  1159,  2492,   272,  4126,  1034,   298, 11285, 28742,\n",
       "         28713,  3487, 20415, 28725,  3210,   821,   272,  4695,  1759,   302,\n",
       "          2170,  3864,   304, 15832,   272,  8982,  4150,  3837, 28723,   330,\n",
       "           680,  7658,  4372,   682, 17516,   272,  9796,   304,  1917,   697,\n",
       "         14151,   553,   298, 16353,   621,  4681,   298,  3754,   288, 11285,\n",
       "         28742, 28713,  3487, 20415,   684,   272,  8982,  4150,  6400, 28723,\n",
       "            13,    13, 11600,  2572,  3716, 11194,   298,   272,  2996,   829,\n",
       "          3024, 28747,    13,    13, 28740, 28723,  9116,   272,  8982,  4150,\n",
       "         28747,  7337,  6852, 11285, 28742, 28713,  3487, 20415, 28725, 14151,\n",
       "           682,   927,   298,   506,   264, 12230,  3028,   302,   767,   272,\n",
       "          8982,  4150,   682,   936,   614, 28723,   851,   829, 17516, 24568,\n",
       "           356,   264,  3608, 28725,   727, 28725,   304,  4723, 28725,   390,\n",
       "          1162,   390, 23689,   272, 11683,  1274, 28723,    13,    13, 28750,\n",
       "         28723, 23184,  4296,   395,  2663, 28747, 14151,  1659,   506,  3236,\n",
       "           298,  7731,   395,  2848,  3282,   304,  2005,  3338,   298,  1316,\n",
       "           395,  4150,  7394, 28725, 20083,   369,   272,  8982,  4150,   682,\n",
       "          8753,   395,  3376, 28742, 28713, 22245,  2464,   304, 22731, 28723,\n",
       "            13,    13, 28770, 28723,  5158, 21824,  4150,  4162, 28747,  7337,\n",
       "          6852, 11285, 28742, 28713,  3487, 20415, 28725, 14151,   682,   927,\n",
       "           298,  1917,  4118, 10936,   302,   272,  4150, 28725,  1259,   390,\n",
       "           272,  7335, 28725,  8059,   697, 28725,  2887,   304, 16195, 28725,\n",
       "          6290,   442, 15175, 28725,   304,   707,  8326, 14841, 28723,    13,\n",
       "            13, 28781, 28723,  8648,   442, 10221,  1304, 25964, 28747,  1791,\n",
       "         25491, 19411, 11285, 28742, 28713,  3487, 20415, 28725, 14151,   993,\n",
       "           506,  3236,   298,  2231,  5277,   442,  7153,  1304, 25964,   369,\n",
       "          4658,   544,  8598,  4162,   684,   272,  8982,  4150, 28723,    13,\n",
       "            13, 28782, 28723,  8702,   653,   264,  2796,  2838, 28747,   560,\n",
       "          1745,   298,  9087,   272,  2442,   302,  8982, 28725, 14151,   682,\n",
       "           927,   298,  1847,   864,   264,  2623,   442,  2796,  2838,   369,\n",
       "           682, 18951, 11285,   298,  8986,   272,  1951,  1671, 24593,   871,\n",
       "          1132,  6032, 28723,    13,    13,  2675,   455, 28725,   272,  2996,\n",
       "         21165,   356,   272,  7892,  5377,  5944,  3030,   354, 14151,   298,\n",
       "         10130,  1034, 11285, 28742, 28713,  3487, 20415,   304,  5227,   706,\n",
       "           684,   272,  8982,  4150,  6400, 28723,  8469, 28725,   345, 10847,\n",
       "           264,  4150, 28739,  1235,   459, 24329,  2962,   272,  6768, 14151,\n",
       "          3236,   298,  1388,  1159,  2492,   272,  4126,  1034, 28723,  -100,\n",
       "         32001, 13892,    13],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,     1, 32001,  1587,    13,  1976,   460,   396, 16107, 13892,\n",
       "         28723,  1247,   622,   368,  2111,   368,   264,  3638, 28723,  3604,\n",
       "          5541,   349,   298,  4160,   272,  3638,   390,  7152,  3071,   390,\n",
       "           368,   541, 28723,  4023, 13801,   272,  3638,  1073,  3707, 28733,\n",
       "          1403, 28733,  7691,   304, 16205,   574,  5944, 28723,  -100, 32001,\n",
       "          2188,    13,  5238,   264, 12271,   297, 10177, 28723,  -100, 32001,\n",
       "         13892,    13,  9977, 28705, 28740, 28747, 21815,   264,  3817,   354,\n",
       "           272, 12271, 28723,    13, 28737,   622,  4987,   345,  5773,   660,\n",
       "           311, 28739,   325,  1237,  3914, 28731,   390,   272,  3817, 28723,\n",
       "            13,    13,  9977, 28705, 28750, 28747, 13341,   264, 12143,   304,\n",
       "           264, 28445, 28723,    13, 28737,   622,   938,   272, 12143,   345,\n",
       "          4754, 17749, 28739,   325,   532,  1482, 28731,   297,   272,  2169,\n",
       "         28445, 28723,    13,    13,  9977, 28705, 28770, 28747,  1325, 28768,\n",
       "           786,   380,   272, 12143, 28723,    13, 24091,   345,  5773,   660,\n",
       "           311, 28739,   349,   264, 15816, 28725,  4008, 28733,  9701,  3817,\n",
       "         28725,   315,   622, 11365,   786,   380,   345,  4754, 17749, 28739,\n",
       "           297,   272,  2169, 28445,   390,   345,  4754,   267,   611,    13,\n",
       "            13,  9977, 28705, 28781, 28747,  4849,   272, 12271, 28723,    13,\n",
       "          8479, 28725,   486, 27698,   272,  3817,   304,   272, 11365,   786,\n",
       "           601, 12143, 28725,   272, 12271,   297, 10177,   349, 28747,   345,\n",
       "          5773,   660,   311,  7952,   611,    13,    13, 10202,  2500, 28747,\n",
       "           851, 12271,   349,  3588, 28725,   847,  6461,  6789,  4714, 28725,\n",
       "           304,  6695,  1002,   272,  4979, 11365,   786,   352,   302,   272,\n",
       "         12143,   345,  4754, 17749, 28739,   297,   272,  2169, 28445,   354,\n",
       "           272,  3817,   345,  5773,   660,   311,   611,   661,  7254,  1002,\n",
       "           298,   345,  1014,  3914,  7825, 28739,   297,  4300, 28723,  -100,\n",
       "         32001, 13892,    13]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "class OpenOrcaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        tokenizer=None,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset_item = self.dataset[idx]\n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": dataset_item[\"system_prompt\"]},\n",
    "            {\"role\": \"user\", \"content\": dataset_item[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": dataset_item[\"response\"]},\n",
    "        ]\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        for key in inputs.keys():\n",
    "            inputs[key] = inputs[key].squeeze(0)\n",
    "        # print(inputs['input_ids'].shape)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "train_elements = 1000\n",
    "valid_elements = 100\n",
    "\n",
    "train_dataset = OpenOrcaDataset(\n",
    "    dataset=dataset[:train_elements],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "valid_dataset = OpenOrcaDataset(\n",
    "    dataset=dataset[train_elements : train_elements + valid_elements],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "pad_datacollator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    # padding=True,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=pad_datacollator,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=pad_datacollator,\n",
    ")\n",
    "\n",
    "next(iter(train_dataloader))\n",
    "next(iter(valid_dataloader))\n",
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.85s/it]\n"
     ]
    }
   ],
   "source": [
    "device = 2\n",
    "student_model = MistralForCausalLM.from_pretrained(model_name, device_map={\"\": device})\n",
    "student_model = student_model.eval()\n",
    "student_model = student_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_layer = 17\n",
    "student_mlp = student_model.model.layers[distill_layer].mlp\n",
    "\n",
    "student_gate_proj = student_mlp.gate_proj.weight.data\n",
    "student_up_proj = student_mlp.up_proj.weight.data\n",
    "student_down_proj = student_mlp.down_proj.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnN0lEQVR4nO3df3TU1Z3/8dckJJNEkkkiJAEyYAAbQeSHyI9gF/CQmiJrm57+wXLchbqKxy50pfjVgm1pt57TuMvSslUUratxd4+A2oIuAjUGgaWEnxIlClGBAgoTEMhMfkASMvf7BzJlJIFMSHIzM8/HOXPa+XzuZz7vyT2ceXk/93M/DmOMEQAAgCUxtgsAAADRjTACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwKoetgtoC7/fr+PHjys5OVkOh8N2OQAAoA2MMaqpqVHfvn0VE9P6+EdYhJHjx4/L7XbbLgMAALTDsWPHlJ2d3er+sAgjycnJki5+mZSUFMvVAACAtvD5fHK73YHf8daERRi5dGkmJSWFMAIAQJi51hQLJrACAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwKiyeTdNZDp6q1bEz9XKnJ2lQ7562ywEAICpFbRg5eKpWv99ySKfrGnXjDfGaPXEggQQAAAui9jLNsTP1Ol3XqCFZyTpd16jPz56zXRIAAFEpasOIOz1JN94Qr/2eGt14Q7yy0xJtlwQAQFSK2ss0g3r31OyJA/X52XPKTkvkEg0AAJZEbRiRLgYSQggAAHZF7WUaAADQPRBGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYFVIYKSoq0pgxY5ScnKyMjAwVFhaqsrLyqscUFxfL4XAEvRISEq6raAAAEDlCCiObN2/WnDlztH37dpWUlKipqUl333236urqrnpcSkqKTpw4EXgdOXLkuooGAACRo0cojTds2BD0vri4WBkZGdqzZ48mTpzY6nEOh0NZWVntqxAAAES065oz4vV6JUnp6elXbVdbW6sBAwbI7Xbru9/9rj766KOrtm9oaJDP5wt6AQCAyNTuMOL3+zVv3jzdeeedGjZsWKvtcnNz9dJLL+nNN9/U//zP/8jv92vChAn6/PPPWz2mqKhILpcr8HK73e0tEwAAdHMOY4xpz4E//OEPtX79em3dulXZ2dltPq6pqUlDhgzRjBkz9OSTT7bYpqGhQQ0NDYH3Pp9PbrdbXq9XKSkp7SkXAAB0MZ/PJ5fLdc3f75DmjFwyd+5crV27Vlu2bAkpiEhSXFycRo0apc8++6zVNk6nU06nsz2lAQCAMBPSZRpjjObOnavVq1dr48aNysnJCfmEzc3N2rdvn/r06RPysQAAIPKENDIyZ84cvfrqq3rzzTeVnJwsj8cjSXK5XEpMTJQkzZw5U/369VNRUZEk6Ve/+pXGjx+vwYMHq7q6WosXL9aRI0f04IMPdvBXAQAA4SikMPLcc89JkiZPnhy0/eWXX9YPfvADSdLRo0cVE/PXAZezZ89q9uzZ8ng8SktL0+jRo7Vt2zYNHTr0+ioHAAARod0TWLtSWyfAAACA7qOtv988mwYAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWBVSGCkqKtKYMWOUnJysjIwMFRYWqrKy8prHvf7667rllluUkJCg2267TevWrWt3wQAAILKEFEY2b96sOXPmaPv27SopKVFTU5Puvvtu1dXVtXrMtm3bNGPGDD3wwAPau3evCgsLVVhYqIqKiusuHgAAhD+HMca09+BTp04pIyNDmzdv1sSJE1tsM336dNXV1Wnt2rWBbePHj9fIkSO1fPnyNp3H5/PJ5XLJ6/UqJSWlveUCAIAu1Nbf7+uaM+L1eiVJ6enprbYpKytTfn5+0LaCggKVlZW1ekxDQ4N8Pl/QCwAARKZ2hxG/36958+bpzjvv1LBhw1pt5/F4lJmZGbQtMzNTHo+n1WOKiorkcrkCL7fb3d4yAQBAN9fuMDJnzhxVVFRo5cqVHVmPJGnhwoXyer2B17Fjxzr8HAAAoHvo0Z6D5s6dq7Vr12rLli3Kzs6+atusrCxVVVUFbauqqlJWVlarxzidTjmdzvaUBgAAwkxIIyPGGM2dO1erV6/Wxo0blZOTc81j8vLyVFpaGrStpKREeXl5oVUKAAAiUkgjI3PmzNGrr76qN998U8nJyYF5Hy6XS4mJiZKkmTNnql+/fioqKpIkPfLII5o0aZKWLFmiadOmaeXKldq9e7deeOGFDv4qAAAgHIU0MvLcc8/J6/Vq8uTJ6tOnT+C1atWqQJujR4/qxIkTgfcTJkzQq6++qhdeeEEjRozQG2+8oTVr1lx10isAAIge17XOSFdhnREAAMJPl6wzAgAAcL0IIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAq0IOI1u2bNG9996rvn37yuFwaM2aNVdtv2nTJjkcjiteHo+nvTUDAIAIEnIYqaur04gRI7Rs2bKQjqusrNSJEycCr4yMjFBPDQAAIlCPUA+YOnWqpk6dGvKJMjIylJqaGvJxAAAgsnXZnJGRI0eqT58++ta3vqU///nPV23b0NAgn88X9AIAAJGp08NInz59tHz5cv3hD3/QH/7wB7ndbk2ePFnvv/9+q8cUFRXJ5XIFXm63u7PLBAAAljiMMabdBzscWr16tQoLC0M6btKkSerfv7/++7//u8X9DQ0NamhoCLz3+Xxyu93yer1KSUlpb7kAAKAL+Xw+uVyua/5+hzxnpCOMHTtWW7dubXW/0+mU0+nswooAAIAtVtYZKS8vV58+fWycGgAAdDMhj4zU1tbqs88+C7w/fPiwysvLlZ6erv79+2vhwoX64osv9F//9V+SpKVLlyonJ0e33nqrzp8/rxdffFEbN27UO++803HfAgAAhK2Qw8ju3bt11113Bd7Pnz9fkjRr1iwVFxfrxIkTOnr0aGB/Y2OjHn30UX3xxRdKSkrS8OHD9e677wZ9BgAAiF7XNYG1q7R1AgwAAOg+2vr7zbNpAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABW9bBdgE0HT9Xq2Jl6udOTNKh3T9vlAAAQlaI2jBw8Vavfbzmk03WNuvGGeM2eOJBAAgCABVF7mebYmXqdrmvUkKxkna5r1Odnz9kuCQCAqBS1YcSdnqQbb4jXfk+NbrwhXtlpibZLAgAgKkXtZZpBvXtq9sSB+vzsOWWnJXKJBgAAS6I2jEgXAwkhBAAAu6L2Mg0AAOgeCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrovqpvQdP1erYmXq505N4ei8AAJZEbRg5eKpWv99ySEfP1Csu1qH778zR5NwM22UBABB1ovYyzbEz9Tp6pl7e+kZ9fLxGL//5sA6eqrVdFgAAUSdqw4g7PUlxsQ6drGlU7+R4NTUbfX72nO2yAACIOlEbRgb17qn778zR0L7JSk2KV//0JGWnJdouCwCAqBO1c0YkaXJuhtzpSfr87DllpyUyiRUAAAtCHhnZsmWL7r33XvXt21cOh0Nr1qy55jGbNm3S7bffLqfTqcGDB6u4uLgdpXaOQb17atI3ehNEAACwJOQwUldXpxEjRmjZsmVtan/48GFNmzZNd911l8rLyzVv3jw9+OCD+tOf/hRysQAAIPKEfJlm6tSpmjp1apvbL1++XDk5OVqyZIkkaciQIdq6dat++9vfqqCgINTTAwCACNPpE1jLysqUn58ftK2goEBlZWWdfeo2OXiqVpsqT3JbLwAAlnT6BFaPx6PMzMygbZmZmfL5fDp37pwSE6+8g6WhoUENDQ2B9z6fr1Nqu7Tw2em6Rt14Q7xmTxzI3BEAALpYt7y1t6ioSC6XK/Byu92dcp5jZ+p1uq5RQ7KSdbqukXVGAACwoNPDSFZWlqqqqoK2VVVVKSUlpcVREUlauHChvF5v4HXs2LFOqc2dnqQbb4jXfk+NbrwhnnVGAACwoNMv0+Tl5WndunVB20pKSpSXl9fqMU6nU06ns7NL06DePfXtYVna94VXt/VzcYkGAAALQh4Zqa2tVXl5ucrLyyVdvHW3vLxcR48elXRxVGPmzJmB9g8//LAOHTqkxx9/XAcOHNCzzz6r1157TT/+8Y875htch4OnarWhwqMPP/dqQ4WHSawAAFgQchjZvXu3Ro0apVGjRkmS5s+fr1GjRmnRokWSpBMnTgSCiSTl5OTo7bffVklJiUaMGKElS5boxRdf7Ba39TJnBAAA+xzGGGO7iGvx+XxyuVzyer1KSUnpsM89eKpWS/5UqRO+8+qTkqBHC3K5VAMAQAdp6+93t7ybpks5JMdX/wsAALpeVIeRY2fqVV3fpOy0RFXXN3GZBgAAC6L6qb2SdPR0vT76wqfkhB4KgytWAABEnKgeGTnhPaf6pguKiZHqmy7I4ztvuyQAAKJOVI+MnKppVH3DBfmNFOOQTtU0XPsgAADQoaJ6ZOQix1dzV5nBCgCADVE9MtI72akbnLFq9hvFxjjUO7nzV30FAADBonpkpI8rQYnxPXTBb+RwOJjACgCABVEdRiSpd0+nkhN66HxTs/74/ucsCQ8AQBeL6jDiTk9Sj9iLE1cbLvj1SVWtdv3ljO2yAACIKlEdRgb17qnstBvk90vNfqPa8xdU6amxXRYAAFElqiewSpIrMU4xDsnhkIzhnhoAALpa1IeR9Bvi5De6uNaIpNSkONslAQAQVaI+jJypa5LDIcV+9b66vslqPQAARJuoDyPec01qvuyOXsIIAABdK+rDiCsxTnGxDhm/kV9S7XnCCAAAXSmq76aRpMm5vZWS0EMXvpo3svMvZ7Sp8qTtsgAAiBqEkdwMfSMzOfC++twF/W/5cYsVAQAQXaI+jEjSucbmoPeHvmQVVgAAugphRFLDBX/Q+1O1jZYqAQAg+hBGJPVLSwx6X+U9x7wRAAC6CGFE0t+PH6C4y/4STX7p1e1H7BUEAEAUIYzo4iTWlMTglVc/+MJrqRoAAKILYeQrrq+FkdM1DTp4iomsAAB0NsLIVwpH9Qt6f8FIP1+9z1I1AABED8LIV/55yjeUGBf85yg7dIbREQAAOhlh5DJ9XAlB740YHQEAoLMRRi4ze+LAK7YxOgIAQOcijFxmxtgB6tUzeCKrkTS7eJedggAAiAKEka959O7cK7YdOl2vB18hkAAA0BkII1/T0uiIJL27/6R+V/qJhYoAAIhshJEWtDQ6Ikm/KfmU+SMAAHQwwkgLZowdoLyB6S3uu2fp5i6uBgCAyEYYacWKh/KUc2PSFdsbmqVbfrbOQkUAAEQmwshVvPfYXUq/4cr5I+cvGN38xNtcsgEAoAMQRq7h/Z/fLWes44rtTX5pypLN+uVbFRaqAgAgchBG2mDdvImt/qGKtx3RXYvf69J6AACIJISRNhjUu6dKHp2kuFb+WodP1+uWn6/nsg0AAO1AGGmjQb176tNfT1NSK4nkfJNfU5ZsZnE0AABCRBgJ0cdPTlVGcnyr+9/df1KDFr7NAmkAALQRYaQddv70W8ofktHq/mZzcYG0oYvWa1PlyS6sDACA8OMwxhjbRVyLz+eTy+WS1+tVSkqK7XICNlWe1IPFu3ThGn/BxLgYLbp3qGaMHdA1hQEA0A209febMNIB7lr8ng6frr9mux4x0t+PH6BffmdYF1QFAIBdhJEutmLnES1aU6Emf9va5w/J0IuzxnRuUQAAWEQYseSXb1WoeNuRNrfv6YzVQxMH6p+nfKMTqwIAoOsRRix78JVdend/aJNXs1Kceur7wzU5t/XJsQAAhAvCSDfx41V7tXrv8ZCPG9grSb+fNUaDevfshKoAAOh8hJFu5neln2jZxk/V0Bz6sYyYAADCEWGkm9pUeVI/eeMDVdU0tuv4hLgY/d0YN3fkAAC6PcJIGPjlWxVaseNIu0ZLLmECLACguyKMhJkHX9ml0v0ndb2dwSUdAEB3QRgJYzNeKNP2Q2euO5hIUlpSnB7/di6rvwIAuhxhJEI8+Moubdx/Um1cS+2aHJJGZLu0Zu43O+gTAQBoGWEkAv2u9BM9v+mg6tq6zGsbMSkWANAZOjWMLFu2TIsXL5bH49GIESP09NNPa+zYsS22LS4u1v333x+0zel06vz5820+H2GkZT9etVf/+8FxXejYbCJJiot16G+H99Fvp4/q+A8HAESFtv5+9wj1g1etWqX58+dr+fLlGjdunJYuXaqCggJVVlYqI6PlSZMpKSmqrKwMvHc4HKGeFi347fRRQWGhIy/pNDUbrd57PGjBthiHNC4nXSseyuuAMwAAcFHIIyPjxo3TmDFj9Mwzz0iS/H6/3G63fvSjH2nBggVXtC8uLta8efNUXV3d7iIZGWmfzhw5uRx38AAAWtIpIyONjY3as2ePFi5cGNgWExOj/Px8lZWVtXpcbW2tBgwYIL/fr9tvv12//vWvdeutt7bavqGhQQ0NDUFfBqH7+sjJwVO1ml28S4dO13foeTy+Bv3g5V1B22Id0l238GRiAMC1hRRGvvzySzU3NyszMzNoe2Zmpg4cONDiMbm5uXrppZc0fPhweb1e/fu//7smTJigjz76SNnZ2S0eU1RUpH/5l38JpTS0waDePbXxsbuCtnXWpNhmI727/6RuWvD2Fft47g4A4HIhXaY5fvy4+vXrp23btikv76/zBh5//HFt3rxZO3bsuOZnNDU1aciQIZoxY4aefPLJFtu0NDLidru5TNNFVuw8on9df0DV5y506XlZEwUAIkunXKbp1auXYmNjVVVVFbS9qqpKWVlZbfqMuLg4jRo1Sp999lmrbZxOp5xOZyiloQPNGDvgikDQWZd4Lne2vkkL/1ihhX+sCNoeGyPdlcslHwCIVCGFkfj4eI0ePVqlpaUqLCyUdHECa2lpqebOndumz2hubta+fft0zz33hFws7GnpEo/U8YuytaTZ3/olH0ZTACD8hXw3zapVqzRr1iw9//zzGjt2rJYuXarXXntNBw4cUGZmpmbOnKl+/fqpqKhIkvSrX/1K48eP1+DBg1VdXa3FixdrzZo12rNnj4YOHdqmc3I3TXi53icTdxQeIggAdnXaOiPTp0/XqVOntGjRInk8Ho0cOVIbNmwITGo9evSoYmJiAu3Pnj2r2bNny+PxKC0tTaNHj9a2bdvaHEQQfibnZmjHT7/V4r6OfO7OtdQ2NOs3JZ/qNyWfXrGPVWcBoPtgOXh0C121JkpbMaoCANePZ9MgInSXSz6XYyVaAGgbwggiXncbTbmES0AAcBFhBFHtl29VaMWOI2potl3JlXgIIYBoQRgBWtFZq852FNZVARApCCNAO3XXyz+XY0l9AOGAMAJ0gq5YibYjMMkWQHdAGAEs6O6XgC7HRFsAnY0wAnRDK3Ye0a/f3q+a7jiztgUEFgDXgzAChKHuuK7KtXB3EIDWEEaACNWVS+p3FO4QAqITYQSIUgdP1WrWf+7Q59XnbZcSMp7CDEQWwgiAVoXTRNuvY5QFCB+EEQDXJZwDi8TkW6A7IIwA6HQrdh7Rv64/oOpzF2yX0m7ZqQl65YFxLCAHdALCCIBuIRzvEPo6h6QR2S6tmftN26UAYYUwAiCshMMy/NdCaAGCEUYARJxIGGWRmISL6EEYARC1wn3y7SVMwkW4I4wAwDWE4wJyLXHGOjRjXH9CC7odwggAdJDCZ7aq/HOv7TKuG5eH0NUIIwDQxSIltHB5CB2FMAIA3VCkTMKVCC24NsIIAISxSJmEK3F5KJoRRgAgCkRSaOFBiZGHMAIACIiky0MDeyXp97PGsIR/GCCMAABCFikjLXGxDv3t8D767fRRtkuJaoQRAECniYTQEuOQxuWka8VDebZLiViEEQCAdeF+eYjnDV0fwggAIGyE84MSuVuodYQRAEDEmfFCmcoOnbFdRkiieXSFMAIAiEordh7Rv64/oOpzF2yX0iaRPHeFMAIAQCsOnqrV7OJdOnS63nYp15SV4tRT3x+uybkZtksJGWEEAIDr1J2fN9TTGauHJg7UP0/5hu1SWkUYAQCgk22qPKlHXyvX6bom26VI6n7zUwgjAAB0A7ZHV2yuWEsYAQCgm+vquStdPf+EMAIAQJh78JVd2rj/pDp6+ZXUxDgt/buRnR5KCCMAAESoX75VoRU7jqih+fo+53uj+nbq83sIIwAARJn2zE/pzEDS1t/vHp1ydgAA0OW+fhfNjBfKtP3QGV1t1GH13uMam5OuGWMHdG5xV8HICAAAUeDBV3bp3f0nW9wXK+mdRyd1+B03bf39junQswIAgG7pxVlj9JenpqlfasIV+5ol/b9V5V1e0yWEEQAAosifF0xpMZDs+8Krg6dqLVREGAEAIOr8ecEUJcUHR4BmI63bd8JKPYQRAACi0LP3jVaPy1JAjEM6W9dopRbCCAAAUWhybobuHdE38N4YKTUpzkot3NoLAECUciXGq0fMxQfsGUnV9XYe+EcYAQAgihkj+fXXQGIDYQQAgCiVm5Ws1KR4Nfv9io2JUW5WspU6CCMAAESpsTnpGjcwXR7veWW5EjTmpnQrdRBGAACIUoN699Sjd+fq87PnlJ2W2OErsLYVYQQAgCg2qHdPayHkEm7tBQAAVrUrjCxbtkw33XSTEhISNG7cOO3cufOq7V9//XXdcsstSkhI0G233aZ169a1q1gAABB5Qg4jq1at0vz58/WLX/xC77//vkaMGKGCggKdPNnykwC3bdumGTNm6IEHHtDevXtVWFiowsJCVVRUXHfxAAAg/DmMMSHdVjxu3DiNGTNGzzzzjCTJ7/fL7XbrRz/6kRYsWHBF++nTp6uurk5r164NbBs/frxGjhyp5cuXt+mcbX0EMQAA6D7a+vsd0shIY2Oj9uzZo/z8/L9+QEyM8vPzVVZW1uIxZWVlQe0lqaCgoNX2ktTQ0CCfzxf0AgAAkSmkMPLll1+qublZmZmZQdszMzPl8XhaPMbj8YTUXpKKiorkcrkCL7fbHUqZAAAgjHTLu2kWLlwor9cbeB07dsx2SQAAoJOEtM5Ir169FBsbq6qqqqDtVVVVysrKavGYrKyskNpLktPplNPpDKU0AAAQpkIaGYmPj9fo0aNVWloa2Ob3+1VaWqq8vLwWj8nLywtqL0klJSWttgcAANEl5BVY58+fr1mzZumOO+7Q2LFjtXTpUtXV1en++++XJM2cOVP9+vVTUVGRJOmRRx7RpEmTtGTJEk2bNk0rV67U7t279cILL3TsNwEAAGEp5DAyffp0nTp1SosWLZLH49HIkSO1YcOGwCTVo0ePKibmrwMuEyZM0Kuvvqqf/exneuKJJ3TzzTdrzZo1GjZsWMd9CwAAELZCXmfEBtYZAQAg/LT19zssHpR3KS+x3ggAAOHj0u/2tcY9wiKM1NTUSBLrjQAAEIZqamrkcrla3R8Wl2n8fr+OHz+u5ORkORyODvtcn88nt9utY8eOcfknjNBv4Yl+C1/0XXjqDv1mjFFNTY369u0bNJ/068JiZCQmJkbZ2dmd9vkpKSn8AwtD9Ft4ot/CF30Xnmz329VGRC7pliuwAgCA6EEYAQAAVkV1GHE6nfrFL37B0vNhhn4LT/Rb+KLvwlM49VtYTGAFAACRK6pHRgAAgH2EEQAAYBVhBAAAWEUYAQAAVkV1GFm2bJluuukmJSQkaNy4cdq5c6ftkqLGli1bdO+996pv375yOBxas2ZN0H5jjBYtWqQ+ffooMTFR+fn5+vTTT4PanDlzRvfdd59SUlKUmpqqBx54QLW1tUFtPvzwQ/3N3/yNEhIS5Ha79W//9m+d/dUiWlFRkcaMGaPk5GRlZGSosLBQlZWVQW3Onz+vOXPm6MYbb1TPnj31/e9/X1VVVUFtjh49qmnTpikpKUkZGRl67LHHdOHChaA2mzZt0u233y6n06nBgweruLi4s79exHruuec0fPjwwOJXeXl5Wr9+fWA/fRYennrqKTkcDs2bNy+wLWL6zkSplStXmvj4ePPSSy+Zjz76yMyePdukpqaaqqoq26VFhXXr1pmf/vSn5o9//KORZFavXh20/6mnnjIul8usWbPGfPDBB+Y73/mOycnJMefOnQu0+fa3v21GjBhhtm/fbv7v//7PDB482MyYMSOw3+v1mszMTHPfffeZiooKs2LFCpOYmGief/75rvqaEaegoMC8/PLLpqKiwpSXl5t77rnH9O/f39TW1gbaPPzww8btdpvS0lKze/duM378eDNhwoTA/gsXLphhw4aZ/Px8s3fvXrNu3TrTq1cvs3DhwkCbQ4cOmaSkJDN//nzz8ccfm6efftrExsaaDRs2dOn3jRRvvfWWefvtt80nn3xiKisrzRNPPGHi4uJMRUWFMYY+Cwc7d+40N910kxk+fLh55JFHAtsjpe+iNoyMHTvWzJkzJ/C+ubnZ9O3b1xQVFVmsKjp9PYz4/X6TlZVlFi9eHNhWXV1tnE6nWbFihTHGmI8//thIMrt27Qq0Wb9+vXE4HOaLL74wxhjz7LPPmrS0NNPQ0BBo85Of/MTk5uZ28jeKHidPnjSSzObNm40xF/spLi7OvP7664E2+/fvN5JMWVmZMeZiEI2JiTEejyfQ5rnnnjMpKSmBvnr88cfNrbfeGnSu6dOnm4KCgs7+SlEjLS3NvPjii/RZGKipqTE333yzKSkpMZMmTQqEkUjqu6i8TNPY2Kg9e/YoPz8/sC0mJkb5+fkqKyuzWBkk6fDhw/J4PEH943K5NG7cuED/lJWVKTU1VXfccUegTX5+vmJiYrRjx45Am4kTJyo+Pj7QpqCgQJWVlTp79mwXfZvI5vV6JUnp6emSpD179qipqSmo72655Rb1798/qO9uu+02ZWZmBtoUFBTI5/Ppo48+CrS5/DMuteHf5/Vrbm7WypUrVVdXp7y8PPosDMyZM0fTpk274u8bSX0XFg/K62hffvmlmpubgzpHkjIzM3XgwAFLVeESj8cjSS32z6V9Ho9HGRkZQft79Oih9PT0oDY5OTlXfMalfWlpaZ1Sf7Tw+/2aN2+e7rzzTg0bNkzSxb9rfHy8UlNTg9p+ve9a6ttL+67Wxufz6dy5c0pMTOyMrxTR9u3bp7y8PJ0/f149e/bU6tWrNXToUJWXl9Nn3djKlSv1/vvva9euXVfsi6R/b1EZRgBcvzlz5qiiokJbt261XQraIDc3V+Xl5fJ6vXrjjTc0a9Ysbd682XZZuIpjx47pkUceUUlJiRISEmyX06mi8jJNr169FBsbe8WM46qqKmVlZVmqCpdc6oOr9U9WVpZOnjwZtP/ChQs6c+ZMUJuWPuPyc6B95s6dq7Vr1+q9995TdnZ2YHtWVpYaGxtVXV0d1P7rfXetfmmtTUpKCv+F3U7x8fEaPHiwRo8eraKiIo0YMUL/8R//QZ91Y3v27NHJkyd1++23q0ePHurRo4c2b96s3/3ud+rRo4cyMzMjpu+iMozEx8dr9OjRKi0tDWzz+/0qLS1VXl6excogSTk5OcrKygrqH5/Ppx07dgT6Jy8vT9XV1dqzZ0+gzcaNG+X3+zVu3LhAmy1btqipqSnQpqSkRLm5uVyiaSdjjObOnavVq1dr48aNV1wGGz16tOLi4oL6rrKyUkePHg3qu3379gWFyZKSEqWkpGjo0KGBNpd/xqU2/PvsOH6/Xw0NDfRZNzZlyhTt27dP5eXlgdcdd9yh++67L/D/I6bvumyqbDezcuVK43Q6TXFxsfn444/NQw89ZFJTU4NmHKPz1NTUmL1795q9e/caSeY3v/mN2bt3rzly5Igx5uKtvampqebNN980H374ofnud7/b4q29o0aNMjt27DBbt241N998c9CtvdXV1SYzM9P8wz/8g6moqDArV640SUlJ3Np7HX74wx8al8tlNm3aZE6cOBF41dfXB9o8/PDDpn///mbjxo1m9+7dJi8vz+Tl5QX2X7rV8O677zbl5eVmw4YNpnfv3i3eavjYY4+Z/fv3m2XLlnGb6HVYsGCB2bx5szl8+LD58MMPzYIFC4zD4TDvvPOOMYY+CyeX301jTOT0XdSGEWOMefrpp03//v1NfHy8GTt2rNm+fbvtkqLGe++9ZyRd8Zo1a5Yx5uLtvT//+c9NZmamcTqdZsqUKaaysjLoM06fPm1mzJhhevbsaVJSUsz9999vampqgtp88MEH5pvf/KZxOp2mX79+5qmnnuqqrxiRWuozSebll18OtDl37pz5p3/6J5OWlmaSkpLM9773PXPixImgz/nLX/5ipk6dahITE02vXr3Mo48+apqamoLavPfee2bkyJEmPj7eDBw4MOgcCM0//uM/mgEDBpj4+HjTu3dvM2XKlEAQMYY+CydfDyOR0ncOY4zpunEYAACAYFE5ZwQAAHQfhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABW/X9Axq683xpI/wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "U, S, Vh = torch.linalg.svd(\n",
    "    student_gate_proj.to(torch.float32),\n",
    "    # student_up_proj.to(torch.float32),\n",
    "    # student_down_proj.to(torch.float32),\n",
    "    # student_model.model.embed_tokens.weight.data.to(torch.float32),\n",
    "    full_matrices=False,\n",
    ")\n",
    "\n",
    "\n",
    "area = 4\n",
    "\n",
    "plt.scatter(\n",
    "    torch.arange(0, S.shape[0]).numpy(),\n",
    "    S.cpu().numpy(),\n",
    "    s=area,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "threshold = 128\n",
    "plt.scatter(\n",
    "    torch.arange(0, threshold).numpy(),\n",
    "    S[:threshold].cpu().numpy(),\n",
    "    s=area,\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в целом можно заметить что самые основные значения лежат на первых 8-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32002, 4096])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.model.embed_tokens.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131080192"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32002 * 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace to lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14336, 4096])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(U @ torch.diag(S) @ Vh).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 64]) torch.Size([64, 14336])\n"
     ]
    }
   ],
   "source": [
    "# from ebany_research.llm_lora.changed_neox import LinearLora\n",
    "def get_L_R(weights=None, rank=64):\n",
    "    U, S, Vh = torch.linalg.svd(\n",
    "        weights.to(torch.float32),\n",
    "        full_matrices=False,\n",
    "    )\n",
    "\n",
    "    U = U[:, :rank]\n",
    "    S = torch.diag(S[:rank])\n",
    "    Vh = Vh[:rank, :]\n",
    "\n",
    "    L = U @ S\n",
    "    R = Vh\n",
    "    return L, R\n",
    "\n",
    "\n",
    "L, R = get_L_R(student_down_proj)\n",
    "print(L.shape, R.shape)\n",
    "\n",
    "\n",
    "class LinearLora(torch.nn.Module):\n",
    "    def __init__(self, in_dim=768, out_dim=768, r=16, bias=False):\n",
    "        super().__init__()\n",
    "        self.L = torch.nn.Linear(in_dim, r, bias=bias)\n",
    "        self.R = torch.nn.Linear(r, out_dim, bias=bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.R(hidden_states)\n",
    "        hidden_states = self.L(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "def assign_new_weights(original_module, original_weights):\n",
    "    L, R = get_L_R(original_weights, rank=16)\n",
    "    original_module.L.weight.data = L\n",
    "    original_module.R.weight.data = R\n",
    "\n",
    "\n",
    "lora_gate_proj = LinearLora(\n",
    "    in_dim=config.hidden_size,\n",
    "    out_dim=config.intermediate_size,\n",
    "    bias=False,\n",
    ")\n",
    "lora_up_proj = LinearLora(\n",
    "    in_dim=config.hidden_size,\n",
    "    out_dim=config.intermediate_size,\n",
    "    bias=False,\n",
    ")\n",
    "lora_down_proj = LinearLora(\n",
    "    in_dim=config.intermediate_size,\n",
    "    out_dim=config.hidden_size,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "assign_new_weights(\n",
    "    original_module=lora_gate_proj,\n",
    "    original_weights=student_gate_proj,\n",
    ")\n",
    "assign_new_weights(\n",
    "    original_module=lora_up_proj,\n",
    "    original_weights=student_up_proj,\n",
    ")\n",
    "assign_new_weights(\n",
    "    original_module=lora_down_proj,\n",
    "    original_weights=student_down_proj,\n",
    ")\n",
    "\n",
    "student_mlp.gate_proj = lora_gate_proj\n",
    "student_mlp.up_proj = lora_up_proj\n",
    "student_mlp.down_proj = lora_down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7241748480\n",
      "7066472448\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(count_parameters(model))\n",
    "print(count_parameters(student_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:09<00:00,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3748738026618956\n",
      "tensor(10.7497)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def eval_model(model):\n",
    "    total_eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            for eval_batch in tqdm.tqdm(valid_dataloader):\n",
    "                # print(eval_batch)\n",
    "\n",
    "                for key in eval_batch.keys():\n",
    "                    eval_batch[key] = eval_batch[key].to(model.device)\n",
    "\n",
    "                loss = model(\n",
    "                    **eval_batch,\n",
    "                )\n",
    "                total_eval_loss += loss.loss.item()\n",
    "    # break\n",
    "    total_eval_loss = total_eval_loss / len(valid_dataloader)\n",
    "    return total_eval_loss\n",
    "\n",
    "\n",
    "teacher_loss = eval_model(model)\n",
    "print(teacher_loss)\n",
    "print(torch.exp(torch.tensor(teacher_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3757719230651855\n",
      "tensor(10.7593)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "student_model = student_model.eval()\n",
    "\n",
    "student_loss = eval_model(student_model)\n",
    "print(student_loss)\n",
    "print(torch.exp(torch.tensor(student_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# model = model.cpu()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callibrate lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.17.mlp.gate_proj.L.weight\n",
      "model.layers.17.mlp.gate_proj.R.weight\n",
      "model.layers.17.mlp.up_proj.L.weight\n",
      "model.layers.17.mlp.up_proj.R.weight\n",
      "model.layers.17.mlp.down_proj.L.weight\n",
      "model.layers.17.mlp.down_proj.R.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [00:33,  4.90it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.38 GiB. GPU 2 has a total capacty of 39.39 GiB of which 1.93 GiB is free. Process 2465559 has 37.45 GiB memory in use. Of the allocated memory 36.33 GiB is allocated by PyTorch, and 598.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m     total_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_train_loss\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m train_batch\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     21\u001b[0m     train_batch[key] \u001b[38;5;241m=\u001b[39m train_batch[key]\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m accum_iter\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# loss = loss.loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/code/ebany_research/llm_lora/changed_mistral.py:1336\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, max_layer)\u001b[0m\n\u001b[1;32m   1331\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1332\u001b[0m     return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1333\u001b[0m )\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_layer\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1350\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/code/ebany_research/llm_lora/changed_mistral.py:1199\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, max_layer)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1190\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1191\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         use_cache,\n\u001b[1;32m   1197\u001b[0m     )\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1199\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/code/ebany_research/llm_lora/changed_mistral.py:885\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 885\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/code/ebany_research/llm_lora/changed_mistral.py:372\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    375\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    376\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    377\u001b[0m )\n\u001b[1;32m    378\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.38 GiB. GPU 2 has a total capacty of 39.39 GiB of which 1.93 GiB is free. Process 2465559 has 37.45 GiB memory in use. Of the allocated memory 36.33 GiB is allocated by PyTorch, and 598.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def train_model(model):\n",
    "    for param in model.named_parameters():\n",
    "        if \"L\" in param[0] or \"R\" in param[0]:\n",
    "            print(param[0])\n",
    "            param[1].requires_grad_(True)\n",
    "        else:\n",
    "            param[1].requires_grad_(False)\n",
    "\n",
    "    total_train_loss = 0\n",
    "    # model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "    accum_iter = 4\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        for batch_id, train_batch in tqdm.tqdm(enumerate(train_dataloader)):\n",
    "            # print(eval_batch)\n",
    "\n",
    "            for key in train_batch.keys():\n",
    "                train_batch[key] = train_batch[key].to(model.device)\n",
    "\n",
    "            loss = model(\n",
    "                **train_batch,\n",
    "            )\n",
    "            loss = loss.loss / accum_iter\n",
    "            # loss = loss.loss\n",
    "\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            if (batch_id + 1) % accum_iter or (batch_id + 1 == len(train_dataloader)):\n",
    "                # print(loss.item())\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                break\n",
    "            # if (batch_id + 1) % accum_iter or (batch_id + 1 == len(train_dataloader)):\n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "        # break\n",
    "    total_train_loss = total_train_loss / len(train_dataloader)\n",
    "    return total_train_loss\n",
    "\n",
    "\n",
    "train_model(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.466512999534607\n",
      "tensor(11.7813)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "student_model = student_model.eval()\n",
    "\n",
    "student_loss = eval_model(student_model)\n",
    "print(student_loss)\n",
    "print(torch.exp(torch.tensor(student_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

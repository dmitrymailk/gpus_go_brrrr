{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 19.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7241748480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/user-name-goes-here/.local/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from ebany_research.llm_lora.changed_mistral import MistralForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "device = 3\n",
    "model = MistralForCausalLM.from_pretrained(model_name, device_map={\"\": device})\n",
    "model = model.eval()\n",
    "model = model.half()\n",
    "print(count_parameters(model))\n",
    "# model = MistralForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "# )\n",
    "# model = MistralForCausalLM._from_config(config=config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "dataset = load_dataset(\"openaccess-ai-collective/oo-gpt4-filtered\")\n",
    "dataset = dataset[\"train\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cot.86217',\n",
       " 'system_prompt': 'You are an AI assistant that helps people find information.',\n",
       " 'question': \"James runs a TV show and there are 5 main characters and 4 minor characters. He pays the minor characters $15,000 each episode. He paid the major characters three times as much. How much does he pay per episode? Let's be accurate as possible.\",\n",
       " 'response': 'James pays the minor characters $15,000 each episode. Since there are 4 minor characters, he pays them a total of 4 * $15,000 = $60,000 per episode.\\n\\nThe major characters are paid three times as much. So, each major character gets paid 3 * $15,000 = $45,000 per episode.\\n\\nThere are 5 main characters, so he pays them a total of 5 * $45,000 = $225,000 per episode.\\n\\nIn total, James pays $225,000 (major characters) + $60,000 (minor characters) = $285,000 per episode.',\n",
       " '__index_level_0__': 4}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:3', dtype=torch.float16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dataset_id = 4\n",
    "\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": dataset[dataset_id][\"system_prompt\"]},\n",
    "    {\"role\": \"user\", \"content\": dataset[dataset_id][\"question\"]},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": inputs,\n",
    "}\n",
    "# inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "max_layer = 32\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(**inputs, return_dict=True, max_layer=max_layer)\n",
    "\n",
    "    meta = model.model.layers[0].mlp.metadata\n",
    "\n",
    "    diff = meta[\"gate_proj(x)\"] - meta[\"x\"] @ meta[\"gate_proj.weight\"].T\n",
    "\n",
    "\n",
    "diff.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = model.model.layers[0].mlp.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14336, 4096]), torch.Size([1, 82, 4096]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[\"gate_proj.weight\"].shape, meta[\"x\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0654,  0.0341, -0.1154,  ..., -0.0035, -0.0227, -0.0404],\n",
       "         [-0.0159,  0.0358,  0.0111,  ..., -0.0369, -0.2202, -0.0027],\n",
       "         [-0.0570, -0.0512, -0.0150,  ...,  0.0484, -0.1598, -0.0205],\n",
       "         ...,\n",
       "         [-0.0699,  0.0356, -0.0950,  ..., -0.0266,  0.0304, -0.0388],\n",
       "         [ 0.0313, -0.0826, -0.0411,  ..., -0.0871, -0.0525, -0.1994],\n",
       "         [-0.0584, -0.0962,  0.0232,  ...,  0.0649,  0.0434, -0.0825]]],\n",
       "       device='cuda:3')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[\"x\"] @ meta[\"gate_proj.weight\"].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class LinearLora(torch.nn.Module):\n",
    "    def __init__(self, in_dim=768, out_dim=768, r=16, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_h_to_4h = torch.nn.Linear(in_dim, r, bias=bias)\n",
    "\n",
    "        self.dense_4h_to_h = torch.nn.Linear(r, out_dim, bias=bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
    "\n",
    "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LinearLoraSVD(torch.nn.Module):\n",
    "    def __init__(self, in_dim=768, out_dim=768, r=16, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_h_to_4h = torch.nn.Linear(in_dim, r, bias=bias)\n",
    "\n",
    "        self.square = torch.nn.Linear(r, r, bias=bias)\n",
    "\n",
    "        self.dense_4h_to_h = torch.nn.Linear(r, out_dim, bias=bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
    "\n",
    "        hidden_states = self.square(hidden_states)\n",
    "\n",
    "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class NonLinearLora(torch.nn.Module):\n",
    "    def __init__(self, in_dim=768, out_dim=768, r=8, bias=True):\n",
    "        super().__init__()\n",
    "        self.dense_h_to_4h = torch.nn.Linear(in_dim, r, bias=bias)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense_4h_to_h = torch.nn.Linear(r, out_dim, bias=bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Lora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=768,\n",
    "        out_dim=768,\n",
    "        r=8,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.A = Parameter(\n",
    "            torch.empty(\n",
    "                (in_dim, r),\n",
    "            ),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.B = Parameter(\n",
    "            torch.empty(\n",
    "                (r, out_dim),\n",
    "            ),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.B.data.normal_(mean=0.0, std=0.02)\n",
    "        self.A.data.normal_(mean=0.0, std=0.02)\n",
    "        self.lora = LinearLora(\n",
    "            in_dim=in_dim,\n",
    "            out_dim=out_dim,\n",
    "            r=r,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.lora2 = LinearLora(\n",
    "            in_dim=in_dim,\n",
    "            out_dim=out_dim,\n",
    "            r=r,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states @ (self.A @ self.B) + self.lora(hidden_states)\n",
    "        # hidden_states = self.lora(hidden_states) + self.lora2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# lora_model = LinearLoraSVD(\n",
    "#     in_dim=config.hidden_size,\n",
    "#     out_dim=config.intermediate_size,\n",
    "#     r=64*2,\n",
    "#     bias=True,\n",
    "# )\n",
    "# lora_model = NonLinearLora(\n",
    "#     in_dim=config.hidden_size,\n",
    "#     out_dim=config.intermediate_size,\n",
    "#     r=64*2*2,\n",
    "\n",
    "\n",
    "#     bias=True,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "\n",
    "amount_epochs = 10000\n",
    "\n",
    "\n",
    "for layer_pos in range(max_layer):\n",
    "    meta = model.model.layers[layer_pos].mlp.metadata\n",
    "    # lora_model = LinearLora(\n",
    "    #     in_dim=config.hidden_size,\n",
    "    #     out_dim=config.intermediate_size,\n",
    "    #     r=16,\n",
    "    #     bias=True,\n",
    "    # )\n",
    "    lora_model = Lora(\n",
    "        in_dim=config.hidden_size,\n",
    "        out_dim=config.intermediate_size,\n",
    "        r=64,\n",
    "        bias=True,\n",
    "    )\n",
    "    lora_model.to(model.device)\n",
    "    optimizer = torch.optim.Adam(lora_model.parameters(), lr=0.001)\n",
    "\n",
    "    for step in range(amount_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        lora_result = lora_model(meta[\"x\"])\n",
    "        labels = torch.vstack([meta[\"gate_proj(x)\"]])\n",
    "\n",
    "        predicts = torch.vstack([lora_result])\n",
    "        loss = mse_loss(\n",
    "            predicts,\n",
    "            labels,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Layer {layer_pos} Step: {step} - {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_params = config.hidden_size * config.intermediate_size\n",
    "new_params = (config.hidden_size * 64 + 64 * config.intermediate_size) * 2\n",
    "\n",
    "print(original_params)\n",
    "print(new_params)\n",
    "print(original_params / new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = torch.svd_lowrank(meta[\"gate_proj.weight\"], q=64, niter=1000, M=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape, S.shape, V.shape, meta[\"gate_proj.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14336, 4096])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(U @ torch.diag(S) @ V.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-26.6719, device='cuda:3')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(U @ torch.diag(S) @ V.T - meta[\"gate_proj.weight\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"x\"] @ (U @ torch.diag(S) @ V.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(540050.2500, device='cuda:3')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(\n",
    "    meta[\"x\"] @ ((U @ torch.diag(S) @ V.T).T),\n",
    "    meta[\"gate_proj(x)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on bigger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# class OpenOrcaDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         dataset=None,\n",
    "#         device=\"cuda:3\",\n",
    "#         tokenizer=None,\n",
    "#     ):\n",
    "#         self.dataset = dataset\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.device = device\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         dataset_item = self.dataset[idx]\n",
    "#         chat = [\n",
    "#             {\"role\": \"system\", \"content\": dataset_item[\"system_prompt\"]},\n",
    "#             {\"role\": \"user\", \"content\": dataset_item[\"question\"]},\n",
    "#         ]\n",
    "#         inputs = self.tokenizer.apply_chat_template(\n",
    "#             chat,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True,\n",
    "#         )\n",
    "#         inputs = self.tokenizer(\n",
    "#             inputs,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).to(self.device)\n",
    "#         for key in inputs.keys():\n",
    "#             inputs[key] = inputs[key].squeeze(0)\n",
    "#         # print(inputs['input_ids'].shape)\n",
    "#         return inputs\n",
    "\n",
    "\n",
    "# train_dataset = OpenOrcaDataset(\n",
    "#     dataset=dataset[\"train\"],\n",
    "#     device=model.device,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# pad_datacollator = DataCollatorWithPadding(\n",
    "#     tokenizer=tokenizer,\n",
    "#     padding=True,\n",
    "# )\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     dataset=train_dataset,\n",
    "#     batch_size=1,\n",
    "#     collate_fn=pad_datacollator,\n",
    "# )\n",
    "# next(iter(train_dataloader))\n",
    "# # train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  8.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "dataset_size = 200\n",
    "max_layer = 16\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "for dataset_id in tqdm.tqdm(range(dataset_size)):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": dataset[dataset_id][\"system_prompt\"]},\n",
    "        {\"role\": \"user\", \"content\": dataset[dataset_id][\"question\"]},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        # padding=\"max_length\",\n",
    "        # max_length=4096,\n",
    "    ).to(model.device)\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": inputs,\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        model(**inputs, return_dict=True, max_layer=max_layer)\n",
    "\n",
    "    for layer_pos in range(max_layer):\n",
    "        meta = model.model.layers[layer_pos].mlp.metadata\n",
    "        new_meta = {}\n",
    "        if layer_pos == max_layer - 1:\n",
    "            for key in meta.keys():\n",
    "                new_meta[key] = meta[key].to(\"cpu\").clone()\n",
    "            dataset[dataset_id][layer_pos] = new_meta\n",
    "        del model.model.layers[layer_pos].mlp.metadata\n",
    "    # print(dataset[dataset_id][0]['x'].shape)\n",
    "    # break\n",
    "\n",
    "    new_dataset.append(dataset[dataset_id])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 90, 4096])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[1][layer_pos][\"x\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTREMELLY LARGE AND LONG\n",
    "# dataset_name = \"./ebany_research/llm_lora/openorca_train_tensors_layer_0.pt\"\n",
    "dataset_name = f\"./ebany_research/llm_lora/openorca_train_tensors_layer_{layer_pos}.pt\"\n",
    "torch.save(new_dataset, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### free gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model = model.cpu()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "new_dataset = torch.load(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0405, -1.0215,  0.2246,  ..., -0.0882, -1.3809, -0.0951],\n",
       "         [ 0.3958, -1.0752,  0.9307,  ..., -0.3438, -0.7207, -0.5972],\n",
       "         [-0.0451,  0.1144, -0.0617,  ...,  0.0320,  0.0298,  0.0069],\n",
       "         ...,\n",
       "         [ 0.1104, -0.7886, -0.5244,  ..., -0.5146, -0.3328, -1.0049],\n",
       "         [-0.1449, -0.4019,  0.2666,  ..., -0.2749, -0.7480, -0.5869],\n",
       "         [ 0.1627, -0.3955,  0.8164,  ..., -0.8057,  0.8018, -0.9912]]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0][layer_pos][\"gate_proj(x)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 14336])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0][0][\"gate_proj(x)\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[ 2.3718e-01,  5.1094e+00,  1.5576e-01,  ..., -1.5674e+00,\n",
       "            1.3904e-01, -3.6670e-01],\n",
       "          [ 7.3926e-01,  2.5039e+00,  2.8047e+00,  ..., -1.6943e+00,\n",
       "           -5.0928e-01, -5.3027e-01],\n",
       "          [-1.5335e-02, -2.6660e-01,  1.2817e-01,  ...,  1.1652e-01,\n",
       "            3.5934e-03,  4.9019e-03],\n",
       "          ...,\n",
       "          [ 1.0332e+00,  2.0879e+00, -1.1895e+00,  ..., -1.1006e+00,\n",
       "           -1.7881e+00, -1.8613e+00],\n",
       "          [ 6.5039e-01,  2.7402e+00,  1.9951e+00,  ...,  8.7988e-01,\n",
       "           -2.3887e+00,  4.4067e-01],\n",
       "          [-2.3672e+00,  5.0508e+00, -1.0488e+00,  ...,  1.0723e+00,\n",
       "           -5.6299e-01,  7.3242e-01]]], dtype=torch.float16),\n",
       " 'gate_proj(x)': tensor([[[ 0.0397, -1.0205,  0.2244,  ..., -0.0872, -1.3809, -0.0957],\n",
       "          [ 0.3972, -1.0752,  0.9297,  ..., -0.3442, -0.7192, -0.5991],\n",
       "          [-0.0451,  0.1144, -0.0617,  ...,  0.0320,  0.0298,  0.0069],\n",
       "          ...,\n",
       "          [-0.6221, -0.2135,  0.0919,  ..., -0.2054, -0.2878, -0.2283],\n",
       "          [-0.5415, -0.5093,  0.3442,  ...,  0.1261, -0.4583, -0.5630],\n",
       "          [ 0.1852, -1.2109,  1.4990,  ..., -0.6860,  0.3977, -0.7983]]],\n",
       "        dtype=torch.float16),\n",
       " 'gate_proj.weight': tensor([[ 0.0008, -0.0028, -0.0017,  ...,  0.0040,  0.0031,  0.0011],\n",
       "         [-0.0025,  0.0012, -0.0009,  ...,  0.0003, -0.0002,  0.0058],\n",
       "         [-0.0013, -0.0021, -0.0004,  ..., -0.0009,  0.0020,  0.0010],\n",
       "         ...,\n",
       "         [ 0.0031, -0.0009,  0.0009,  ..., -0.0030, -0.0008, -0.0013],\n",
       "         [ 0.0011,  0.0006, -0.0005,  ..., -0.0015, -0.0029, -0.0030],\n",
       "         [-0.0034,  0.0029, -0.0046,  ..., -0.0001,  0.0006,  0.0004]],\n",
       "        dtype=torch.float16)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TensorsOpenOrcaDataset(Dataset):\n",
    "    def __init__(self, dataset=None, device=\"cuda:3\", layer: int = 0):\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.layer = layer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset_item = self.dataset[idx]\n",
    "        inputs = dataset_item[self.layer]\n",
    "\n",
    "        # for key in inputs.keys():\n",
    "        #     inputs[key] = inputs[key].squeeze(0)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "train_tensor_dataset = TensorsOpenOrcaDataset(\n",
    "    dataset=new_dataset[:100], layer=layer_pos\n",
    ")\n",
    "valid_tensor_dataset = TensorsOpenOrcaDataset(\n",
    "    dataset=new_dataset[100:], layer=layer_pos\n",
    ")\n",
    "train_tensor_dataloader = DataLoader(dataset=train_tensor_dataset, batch_size=1)\n",
    "test_tensor_dataloader = DataLoader(dataset=valid_tensor_dataset, batch_size=1)\n",
    "next(iter(valid_tensor_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1]).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - train loss 0.2828603911399841\n",
      "Epoch: 0 - valid loss 0.27942157581448557\n",
      "---\n",
      "Epoch: 1 - train loss 0.28285539388656616\n",
      "Epoch: 1 - valid loss 0.27941677197813986\n",
      "---\n",
      "Epoch: 2 - train loss 0.28285050302743914\n",
      "Epoch: 2 - valid loss 0.27941204637289047\n",
      "---\n",
      "Epoch: 3 - train loss 0.2828456810116768\n",
      "Epoch: 3 - valid loss 0.27940738782286645\n",
      "---\n",
      "Epoch: 4 - train loss 0.28284092277288436\n",
      "Epoch: 4 - valid loss 0.2794027841091156\n",
      "---\n",
      "Epoch: 5 - train loss 0.2828362140059471\n",
      "Epoch: 5 - valid loss 0.2793982243537903\n",
      "---\n",
      "Epoch: 6 - train loss 0.28283155769109725\n",
      "Epoch: 6 - valid loss 0.27939370334148406\n",
      "---\n",
      "Epoch: 7 - train loss 0.2828269371390343\n",
      "Epoch: 7 - valid loss 0.2793892207741737\n",
      "---\n",
      "Epoch: 8 - train loss 0.2828223451972008\n",
      "Epoch: 8 - valid loss 0.27938476458191874\n",
      "---\n",
      "Epoch: 9 - train loss 0.2828177839517593\n",
      "Epoch: 9 - valid loss 0.2793803270161152\n",
      "---\n",
      "Epoch: 10 - train loss 0.2828132367134094\n",
      "Epoch: 10 - valid loss 0.27937590509653093\n",
      "---\n",
      "Epoch: 11 - train loss 0.28280870378017425\n",
      "Epoch: 11 - valid loss 0.2793714925646782\n",
      "---\n",
      "Epoch: 12 - train loss 0.28280417650938033\n",
      "Epoch: 12 - valid loss 0.27936708599328997\n",
      "---\n",
      "Epoch: 13 - train loss 0.2827996560931206\n",
      "Epoch: 13 - valid loss 0.27936267748475074\n",
      "---\n",
      "Epoch: 14 - train loss 0.2827951300144196\n",
      "Epoch: 14 - valid loss 0.279358266890049\n",
      "---\n",
      "Epoch: 15 - train loss 0.2827905949950218\n",
      "Epoch: 15 - valid loss 0.2793538422882557\n",
      "---\n",
      "Epoch: 16 - train loss 0.28278605312108995\n",
      "Epoch: 16 - valid loss 0.2793494077026844\n",
      "---\n",
      "Epoch: 17 - train loss 0.2827814945578575\n",
      "Epoch: 17 - valid loss 0.2793449468910694\n",
      "---\n",
      "Epoch: 18 - train loss 0.2827769154310226\n",
      "Epoch: 18 - valid loss 0.27934046983718874\n",
      "---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 193\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_tensor_dataset):\n\u001b[1;32m    191\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 193\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    195\u001b[0m     predicts \u001b[38;5;241m=\u001b[39m lora_model(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "from ebany_research.llm_lora.changed_neox import LinearLora\n",
    "\n",
    "\n",
    "\n",
    "class NonLinearLora(torch.nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_dim=768, out_dim=768, r=8, bias=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_h_to_4h = torch.nn.Linear(in_dim, r, bias=bias)\n",
    "\n",
    "\n",
    "        self.act = torch.nn.GELU()\n",
    "\n",
    "        self.dense_4h_to_h = torch.nn.Linear(r, out_dim, bias=bias)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "\n",
    "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "\n",
    "\n",
    "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "class Lora(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "        in_dim=768,\n",
    "        out_dim=768,\n",
    "        r=8,\n",
    "        bias=True,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.A = Parameter(torch.empty((in_dim, r)), requires_grad=True)\n",
    "\n",
    "        self.B = Parameter(torch.empty((r, out_dim)), requires_grad=True)\n",
    "\n",
    "\n",
    "        self.A1 = Parameter(torch.empty((in_dim, r)), requires_grad=True)\n",
    "\n",
    "        self.B1 = Parameter(torch.empty((r, out_dim)), requires_grad=True)\n",
    "\n",
    "\n",
    "        self.A.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "        self.B.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "        self.lora = LinearLora(\n",
    "            in_dim=in_dim,\n",
    "            out_dim=out_dim,\n",
    "            r=r,\n",
    "\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.lora2 = LinearLora(\n",
    "            in_dim=in_dim,\n",
    "            out_dim=out_dim,\n",
    "            r=r,\n",
    "\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "\n",
    "        hidden_states = hidden_states @ (self.A @ self.B) + self.lora(hidden_states)\n",
    "\n",
    "        # hidden_states = self.lora(hidden_states) + self.lora2(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LoraSVD(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=768,\n",
    "        out_dim=768,\n",
    "        r=8,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.U = Parameter(torch.empty((out_dim, r)), requires_grad=True)\n",
    "        # self.U = Parameter(torch.empty((in_dim, r)), requires_grad=False)\n",
    "        # self.U_i = Parameter(torch.empty((out_dim, r)), requires_grad=True)\n",
    "\n",
    "        self.D = Parameter(torch.empty((r, r)), requires_grad=True)\n",
    "        # self.D = Parameter(torch.empty((r, r)), requires_grad=False)\n",
    "        # self.D_i = Parameter(torch.empty((r, r)), requires_grad=True)\n",
    "\n",
    "        self.V = Parameter(torch.empty((r, in_dim)), requires_grad=True)\n",
    "        # self.V = Parameter(torch.empty((r, out_dim)), requires_grad=False)\n",
    "        # self.V_i = Parameter(torch.empty((r, in_dim)), requires_grad=True)\n",
    "\n",
    "        # self.U.data.normal_(mean=0.0, std=0.02)\n",
    "        # self.D.data.normal_(mean=0.0, std=0.02)\n",
    "        # self.V.data.normal_(mean=0.0, std=0.02)\n",
    "        # self.lora = LinearLora(\n",
    "        #     in_dim=in_dim,\n",
    "        #     out_dim=out_dim,\n",
    "        #     r=64,\n",
    "\n",
    "        #     bias=True,\n",
    "        # )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states = hidden_states @ (self.A @ self.B) + self.lora(hidden_states)\n",
    "        # hidden_states = (\n",
    "        #     hidden_states\n",
    "        #     @ (\n",
    "        #         ((self.U + self.U_i) / 2)\n",
    "        #         @ ((self.D + self.D_i) / 2)\n",
    "        #         @ ((self.V + self.V_i) / 2)\n",
    "        #     ).T\n",
    "        # )\n",
    "        hidden_states = hidden_states @ (self.U @ self.D @ self.V).T \n",
    "        # hidden_states = self.lora(hidden_states) + self.lora2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "\n",
    "\n",
    "amount_epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "# epoch 306 min train loss  922.\n",
    "\n",
    "r = 16\n",
    "lora_model = LoraSVD(\n",
    "    in_dim=config.hidden_size,\n",
    "    out_dim=config.intermediate_size,\n",
    "    r=r,\n",
    "    bias=True,\n",
    ")\n",
    "# lora_model = Lora(\n",
    "#     in_dim=config.hidden_size,\n",
    "#     out_dim=config.intermediate_size,\n",
    "#     r=r,\n",
    "#     bias=True,\n",
    "\n",
    "\n",
    "# )\n",
    "\n",
    "U, S, Vh = torch.linalg.svd(\n",
    "    new_dataset[0][layer_pos][\"gate_proj.weight\"].to(torch.float32),\n",
    "    full_matrices=False,\n",
    ")\n",
    "# U[:, : r + 1] @ torch.diag(S[: (r + 1)]) @ Vh[: r + 1, :]\n",
    "lora_model.U.data = U[:, :r]\n",
    "lora_model.D.data = torch.diag(S[:r])\n",
    "lora_model.V.data = Vh[:r, :]\n",
    "# lora_model.U_i.data = U[:, :r]\n",
    "# lora_model.D_i.data = torch.diag(S[:r])\n",
    "# lora_model.V_i.data = Vh[:r, :]\n",
    "\n",
    "\n",
    "lora_model.to(device)\n",
    "\n",
    "\n",
    "lora_model.to(torch.float32)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(lora_model.parameters(), lr=0.0000001)\n",
    "\n",
    "\n",
    "for epoch in range(amount_epochs):\n",
    "    train_total_loss = 0\n",
    "    lora_model.train()\n",
    "\n",
    "\n",
    "    for batch_id, batch in enumerate(train_tensor_dataset):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = batch[\"x\"].to(device)\n",
    "        x = x.to(torch.float32)\n",
    "        predicts = lora_model(x)\n",
    "        labels = batch[\"gate_proj(x)\"].to(device)\n",
    "        labels = labels.to(torch.float32)\n",
    "        loss = mse_loss(\n",
    "\n",
    "            predicts,\n",
    "            labels,\n",
    "        )\n",
    "\n",
    "\n",
    "        train_total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(\n",
    "        #     lora_model.parameters(),\n",
    "        #     1.0,\n",
    "        # )\n",
    "\n",
    "\n",
    "        if (batch_id + 1) % 30:\n",
    "\n",
    "            optimizer.step()\n",
    "        # break\n",
    "    # if epoch % 100 == 0:\n",
    "\n",
    "    print(f\"Epoch: {epoch} - train loss {train_total_loss / len(train_tensor_dataset)}\")\n",
    "    lora_model.eval()\n",
    "    valid_total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_id, batch in enumerate(valid_tensor_dataset):\n",
    "\n",
    "            x = batch[\"x\"].to(device)\n",
    "            x = x.to(torch.float32)\n",
    "            predicts = lora_model(x)\n",
    "            labels = batch[\"gate_proj(x)\"].to(device)\n",
    "            labels = labels.to(torch.float32)\n",
    "            loss = mse_loss(\n",
    "                predicts,\n",
    "                labels,\n",
    "            )\n",
    "\n",
    "\n",
    "            valid_total_loss += loss.item()\n",
    "\n",
    "\n",
    "        # break\n",
    "\n",
    "\n",
    "    # if epoch % 100 == 0:\n",
    "\n",
    "    print(f\"Epoch: {epoch} - valid loss {valid_total_loss / len(valid_tensor_dataset)}\")\n",
    "\n",
    "\n",
    "    print(\"---\")\n",
    "\n",
    "    # break\n",
    "\n",
    "\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.592468  , 1.1172432 , 1.0889344 , ..., 0.09039976, 0.08639444,\n",
       "       0.08478527], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 4093, 4094, 4095])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,S.shape[0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1991, 0.0465, 0.0390, 0.0309, 0.0294, 0.0242, 0.0188, 0.0184, 0.0179,\n",
       "        0.0168, 0.0162, 0.0153, 0.0140, 0.0133, 0.0129, 0.0114, 0.0113, 0.0108,\n",
       "        0.0106, 0.0099, 0.0093, 0.0091, 0.0088, 0.0086, 0.0084, 0.0082, 0.0079,\n",
       "        0.0076, 0.0073, 0.0073, 0.0070, 0.0067, 0.0066, 0.0065, 0.0063, 0.0061,\n",
       "        0.0059, 0.0058, 0.0056, 0.0055, 0.0055, 0.0054, 0.0053, 0.0052, 0.0050,\n",
       "        0.0049, 0.0047, 0.0046, 0.0044, 0.0044, 0.0043, 0.0042, 0.0042, 0.0041,\n",
       "        0.0040, 0.0039, 0.0039, 0.0038, 0.0037, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "        0.0034, 0.0033, 0.0033, 0.0033, 0.0032, 0.0031, 0.0031, 0.0030, 0.0030,\n",
       "        0.0029, 0.0029, 0.0029, 0.0028, 0.0027, 0.0027, 0.0027, 0.0026, 0.0026,\n",
       "        0.0026, 0.0026, 0.0025, 0.0024, 0.0024, 0.0024, 0.0024, 0.0023, 0.0023,\n",
       "        0.0023, 0.0023, 0.0022, 0.0022, 0.0021, 0.0021, 0.0021, 0.0021, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0019, 0.0019, 0.0019, 0.0019, 0.0018, 0.0018,\n",
       "        0.0018, 0.0018, 0.0017, 0.0017, 0.0017, 0.0017, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0016, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0014, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0009,\n",
       "        0.0009, 0.0009, 0.0009, 0.0009, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008,\n",
       "        0.0008, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0006,\n",
       "        0.0006, 0.0006, 0.0005, 0.0004, 0.0003, 0.0003])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, Vh = torch.linalg.svd(\n",
    "    new_dataset[0][layer_pos][\"x\"].to(torch.float32),\n",
    "    full_matrices=False,\n",
    ")\n",
    "\n",
    "S[0]**2 / (S**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "\n",
    "area = 5 # 0 to 15 point radii\n",
    "\n",
    "plt.scatter(torch.arange(0,S.shape[0]).numpy(), S.numpy(), s=area, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14336, 4096])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0][layer_pos][\"gate_proj.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14336, 4096]), torch.Size([4096]), torch.Size([4096, 4096]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, Vh = torch.linalg.svd(\n",
    "    new_dataset[0][layer_pos][\"gate_proj.weight\"].to(torch.float32),\n",
    "    full_matrices=False,\n",
    ")\n",
    "U.shape, S.shape, Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 4096]), torch.Size([64, 64]), torch.Size([64, 4096]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.V.data.shape, lora_model.D.data.shape, lora_model.V.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### truncated svd, max singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (14336,4096) | Rank: 16 | U:torch.Size([14336, 4096]) | S:torch.Size([4096]) | Vh:torch.Size([4096, 4096])\n",
      "Reduced Rank: 16 | Num Parameters: 294912\n",
      "L: torch.Size([14336, 16]) | R: torch.Size([16, 4096])\n",
      "true_svd_difference=0.0003874329850077629\n",
      "r=128 lowrank_svd_difference=564.4758911132812\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import random\n",
    "from ebany_research.llm_lora.changed_neox import LinearLora\n",
    "\n",
    "\n",
    "class LoraSVD(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=768,\n",
    "        out_dim=768,\n",
    "        r=8,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.U = Parameter(torch.empty((in_dim, r)), requires_grad=False)\n",
    "        self.U_i = Parameter(torch.empty((in_dim, r)), requires_grad=True)\n",
    "\n",
    "        self.D = Parameter(torch.empty((r, r)), requires_grad=False)\n",
    "        self.D_i = Parameter(torch.empty((r, r)), requires_grad=True)\n",
    "\n",
    "        self.V = Parameter(torch.empty((r, out_dim)), requires_grad=False)\n",
    "        self.V_i = Parameter(torch.empty((r, out_dim)), requires_grad=True)\n",
    "\n",
    "        self.lora = LinearLora(\n",
    "            in_dim=in_dim,\n",
    "            out_dim=out_dim,\n",
    "            r=r,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states = hidden_states @ (self.A @ self.B) + self.lora(hidden_states)\n",
    "        hidden_states = hidden_states @ (\n",
    "            (self.U + self.U_i) @ (self.D + self.D_i) @ (self.V + self.V_i)\n",
    "        )\n",
    "        # hidden_states = self.lora(hidden_states) + self.lora2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "def low_rank_decomposition(\n",
    "    weight,\n",
    "    rank_ratio=0.1,\n",
    "    parameter_ratio=0.15,\n",
    "    remove_criteria=\"max_eigenvalue\",\n",
    "    rank=16,\n",
    "    log_level=\"DEBUG\",\n",
    "    return_dict=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    :param          weight: The matrix to decompose, of shape (H, W)\n",
    "    :param      rank_ratio: rank_of_decomposed_matrix / rank_of_input_weight\n",
    "    :param parameter_ratio: parameter_num_of_decomposed_matrix / (H * W). If specify, override rank_ratio\n",
    "    :param remove_criteria: choose from ['max_eigenvalue', 'random', 'min_eigenvalue']\n",
    "    :param       log_level: choose from ['IGNORE', 'INFO', 'DEBUG']\n",
    "    :param     return_dict: Return a dict if True, else return a tuple (L, R)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"parameter_ratio = rank * (H + W) / (H * W)\"\"\"\n",
    "    \"\"\"rank_ratio = \"\"\"\n",
    "    matrix_dimension = len(weight.size())\n",
    "    assert matrix_dimension == 2, \"Only Support 2D matrix\"\n",
    "    H, W = weight.size()\n",
    "\n",
    "    # Use SVD to decompose a matrix, default full_matrices is False to save parameters\n",
    "    U, S, Vh = torch.linalg.svd(weight, full_matrices=False)\n",
    "    # rank = torch.count_nonzero(S)\n",
    "    # is_full_rank = rank == min(H, W)\n",
    "\n",
    "    # if parameter_ratio is not None:\n",
    "    #     reduced_rank = math.ceil(parameter_ratio * (H * W) / (H + W))\n",
    "    # else:\n",
    "    #     reduced_rank = math.ceil(rank * rank_ratio)\n",
    "    reduced_rank = rank\n",
    "    if remove_criteria == \"max_eigenvalue\":\n",
    "        L = U @ (torch.sqrt(torch.diag(S)[:, 0:reduced_rank]))\n",
    "        R = torch.sqrt(torch.diag(S)[0:reduced_rank, :]) @ Vh\n",
    "    elif remove_criteria == \"random\":\n",
    "        selected_index = random.choices(range(len(S)), k=reduced_rank)\n",
    "        L = U @ (torch.sqrt(torch.diag(S)[:, selected_index]))\n",
    "        R = torch.sqrt(torch.diag(S)[selected_index, :]) @ Vh\n",
    "    elif remove_criteria == \"min_eigenvalue\":\n",
    "        len_s = len(S)\n",
    "        L = U @ (torch.sqrt(torch.diag(S)[:, len_s - reduced_rank :]))\n",
    "        R = torch.sqrt(torch.diag(S)[len_s - reduced_rank :, :]) @ Vh\n",
    "    else:\n",
    "        raise NameError(\"remove criteria not support\")\n",
    "\n",
    "    if log_level == \"DEBUG\":\n",
    "        print(\n",
    "            f\"W: ({H},{W}) | Rank: {rank} | U:{U.shape} | S:{S.shape} | Vh:{Vh.shape}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Reduced Rank: {reduced_rank} | Num Parameters: {(H + W) * reduced_rank}\"\n",
    "        )\n",
    "        print(f\"L: {L.shape} | R: {R.shape}\")\n",
    "\n",
    "    if return_dict:\n",
    "        return {\"L\": L, \"R\": R, \"U\": U, \"S\": S, \"Vh\": Vh, \"reduced_rank\": reduced_rank}\n",
    "    else:\n",
    "        return L, R\n",
    "\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "lora_model = LoraSVD(\n",
    "    in_dim=config.hidden_size,\n",
    "    out_dim=config.intermediate_size,\n",
    "    r=64,\n",
    "    bias=True,\n",
    ")\n",
    "lora_model.to(device)\n",
    "lora_model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = next(iter(valid_tensor_dataset))[\"gate_proj.weight\"].to(device)\n",
    "original_weights = original_weights.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = torch.linalg.svd(original_weights, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = original_weights.shape[0]\n",
    "sigma = 0.32999\n",
    "cutoff = 4 / 3 ** (0.5) * sigma\n",
    "r = torch.where(S > cutoff)[0].max().item()\n",
    "r = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6016e-04, -8.9347e-04, -8.6622e-04,  ..., -3.5067e-04,\n",
       "          2.6375e-04, -3.7556e-04],\n",
       "        [-4.4451e-04, -2.3259e-04,  9.5287e-05,  ..., -4.3446e-04,\n",
       "          5.3174e-04,  1.6125e-03],\n",
       "        [ 4.1949e-04,  1.8159e-03, -1.4118e-04,  ...,  2.5175e-04,\n",
       "         -3.3946e-04, -6.7389e-04],\n",
       "        ...,\n",
       "        [ 1.9930e-04,  7.9790e-04, -9.0183e-04,  ..., -7.6730e-04,\n",
       "          1.2988e-05,  8.4269e-04],\n",
       "        [-7.9907e-05, -5.3753e-06,  1.6677e-04,  ..., -2.9642e-04,\n",
       "         -4.5371e-05, -1.2702e-04],\n",
       "        [ 2.2607e-04,  1.5306e-03,  5.6364e-04,  ...,  6.3548e-04,\n",
       "         -3.3000e-05,  9.6451e-04]], device='cuda:3')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(\n",
    "    U[:, : r + 1] @ torch.diag(S[: (r + 1)]) @ Vh[: r + 1, :],\n",
    "    original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14336, 4096])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(U[:, : r + 1] @ torch.diag(S[: (r + 1)]) @ Vh[: r + 1, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14336, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[:, : r + 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14336, 4096])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m truncated_matrix \u001b[38;5;241m=\u001b[39m U[:, : r \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(S[: (r \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]) \u001b[38;5;241m@\u001b[39m Vh[: r \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(truncated_matrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m diff \u001b[38;5;241m=\u001b[39m \u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncated_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m diff=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3329\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3326\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3328\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu!"
     ]
    }
   ],
   "source": [
    "for r in range(S.shape[0]):\n",
    "    truncated_matrix = U[:, : r + 1] @ torch.diag(S[: (r + 1)]) @ Vh[: r + 1, :]\n",
    "    print(truncated_matrix.shape)\n",
    "\n",
    "    diff = mse_loss(\n",
    "        truncated_matrix,\n",
    "        original_weights,\n",
    "    )\n",
    "\n",
    "    print(f\"r={r} diff={diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5928566455841064,\n",
       " 1.1174074411392212,\n",
       " 1.0885881185531616,\n",
       " 1.0183885097503662,\n",
       " 1.0076931715011597,\n",
       " 1.0032069683074951,\n",
       " 0.9918707609176636,\n",
       " 0.9808992147445679,\n",
       " 0.9594203233718872,\n",
       " 0.9559280276298523,\n",
       " 0.9449878334999084,\n",
       " 0.9441251754760742,\n",
       " 0.9269037246704102,\n",
       " 0.9261745810508728,\n",
       " 0.9165115356445312,\n",
       " 0.9123530983924866,\n",
       " 0.9025701284408569,\n",
       " 0.8972771167755127,\n",
       " 0.8885034918785095,\n",
       " 0.8872251510620117,\n",
       " 0.8825976252555847,\n",
       " 0.8756235241889954,\n",
       " 0.8716121315956116,\n",
       " 0.8686613440513611,\n",
       " 0.8643426299095154,\n",
       " 0.8598101735115051,\n",
       " 0.8585602641105652,\n",
       " 0.8547016382217407,\n",
       " 0.8465468883514404,\n",
       " 0.8428547978401184,\n",
       " 0.8367452025413513,\n",
       " 0.8339047431945801,\n",
       " 0.8310821056365967,\n",
       " 0.8297173380851746,\n",
       " 0.8269672989845276,\n",
       " 0.8246450424194336,\n",
       " 0.8173280358314514,\n",
       " 0.8153272271156311,\n",
       " 0.8134797811508179,\n",
       " 0.8118128180503845,\n",
       " 0.8091875910758972,\n",
       " 0.8035658597946167,\n",
       " 0.8005534410476685,\n",
       " 0.7975896000862122,\n",
       " 0.7968849539756775,\n",
       " 0.7944366931915283,\n",
       " 0.7893564701080322,\n",
       " 0.7858853340148926,\n",
       " 0.7837715744972229,\n",
       " 0.7835241556167603,\n",
       " 0.7819845676422119,\n",
       " 0.7769980430603027,\n",
       " 0.7750994563102722,\n",
       " 0.7726348042488098,\n",
       " 0.7703739404678345,\n",
       " 0.7684022188186646,\n",
       " 0.7664300203323364,\n",
       " 0.7641122341156006,\n",
       " 0.7617926597595215,\n",
       " 0.7607541084289551,\n",
       " 0.7589014172554016,\n",
       " 0.7581141591072083,\n",
       " 0.7563958168029785,\n",
       " 0.7539408802986145,\n",
       " 0.7507840991020203,\n",
       " 0.7494096755981445,\n",
       " 0.7472373247146606,\n",
       " 0.7458621263504028,\n",
       " 0.7436532378196716,\n",
       " 0.7421520948410034,\n",
       " 0.7404975295066833,\n",
       " 0.7378551363945007,\n",
       " 0.7355150580406189,\n",
       " 0.7352234125137329,\n",
       " 0.7336257696151733,\n",
       " 0.732597291469574,\n",
       " 0.7312240600585938,\n",
       " 0.7280887365341187,\n",
       " 0.7265865802764893,\n",
       " 0.7249292135238647,\n",
       " 0.7237201929092407,\n",
       " 0.7219399213790894,\n",
       " 0.7202083468437195,\n",
       " 0.719257116317749,\n",
       " 0.717991828918457,\n",
       " 0.7172157168388367,\n",
       " 0.7136946320533752,\n",
       " 0.7128540873527527,\n",
       " 0.7119123339653015,\n",
       " 0.711364209651947,\n",
       " 0.7108137607574463,\n",
       " 0.7095295786857605,\n",
       " 0.7081347107887268,\n",
       " 0.7075119614601135,\n",
       " 0.7062287330627441,\n",
       " 0.7048563361167908,\n",
       " 0.7036277055740356,\n",
       " 0.7014421820640564,\n",
       " 0.6995660662651062,\n",
       " 0.6975890398025513,\n",
       " 0.6965377926826477,\n",
       " 0.6962352395057678,\n",
       " 0.6948740482330322,\n",
       " 0.6942508816719055,\n",
       " 0.6933507323265076,\n",
       " 0.692685604095459,\n",
       " 0.6913026571273804,\n",
       " 0.6889811754226685,\n",
       " 0.6884909868240356,\n",
       " 0.6882388591766357,\n",
       " 0.686578631401062,\n",
       " 0.6859031915664673,\n",
       " 0.6852887272834778,\n",
       " 0.6834396719932556,\n",
       " 0.6826716661453247,\n",
       " 0.6813976764678955,\n",
       " 0.680397093296051,\n",
       " 0.6798768639564514,\n",
       " 0.6789870858192444,\n",
       " 0.6777352690696716,\n",
       " 0.6766806244850159,\n",
       " 0.6764560341835022,\n",
       " 0.6743595600128174,\n",
       " 0.6735270023345947,\n",
       " 0.6723431348800659,\n",
       " 0.6716905236244202,\n",
       " 0.6714911460876465,\n",
       " 0.6708562970161438,\n",
       " 0.6698088645935059,\n",
       " 0.6695214509963989,\n",
       " 0.6676235198974609,\n",
       " 0.6673620939254761,\n",
       " 0.666537344455719,\n",
       " 0.6656938791275024,\n",
       " 0.6648110747337341,\n",
       " 0.6646736264228821,\n",
       " 0.6626951694488525,\n",
       " 0.6615020632743835,\n",
       " 0.6606176495552063,\n",
       " 0.660098135471344,\n",
       " 0.6592149138450623,\n",
       " 0.6579311490058899,\n",
       " 0.6570395827293396,\n",
       " 0.6564695835113525,\n",
       " 0.6558148860931396,\n",
       " 0.6554353833198547,\n",
       " 0.654982328414917,\n",
       " 0.6544037461280823,\n",
       " 0.6535804271697998,\n",
       " 0.6528536081314087,\n",
       " 0.6522281169891357,\n",
       " 0.6511882543563843,\n",
       " 0.6503733396530151,\n",
       " 0.650091290473938,\n",
       " 0.6494151949882507,\n",
       " 0.6484601497650146,\n",
       " 0.647540271282196,\n",
       " 0.6469152569770813,\n",
       " 0.6460597515106201,\n",
       " 0.6455470323562622,\n",
       " 0.6437244415283203,\n",
       " 0.6435644626617432,\n",
       " 0.6430829763412476,\n",
       " 0.6427310109138489,\n",
       " 0.6421318054199219,\n",
       " 0.6409961581230164,\n",
       " 0.6403363347053528,\n",
       " 0.6401252746582031,\n",
       " 0.6391695737838745,\n",
       " 0.6388764977455139,\n",
       " 0.638164222240448,\n",
       " 0.6376947164535522,\n",
       " 0.6371736526489258,\n",
       " 0.6362867951393127,\n",
       " 0.6358889937400818,\n",
       " 0.635123074054718,\n",
       " 0.6345198154449463,\n",
       " 0.6338881850242615,\n",
       " 0.633352518081665,\n",
       " 0.6325998306274414,\n",
       " 0.6317458152770996,\n",
       " 0.631316602230072,\n",
       " 0.6308591961860657,\n",
       " 0.6302382349967957,\n",
       " 0.6300380229949951,\n",
       " 0.6296090483665466,\n",
       " 0.6287661790847778,\n",
       " 0.6274401545524597,\n",
       " 0.6271229982376099,\n",
       " 0.6265472173690796,\n",
       " 0.6259588003158569,\n",
       " 0.6253101825714111,\n",
       " 0.6248399615287781,\n",
       " 0.6244415044784546,\n",
       " 0.6237080693244934,\n",
       " 0.6232287287712097,\n",
       " 0.6226273775100708,\n",
       " 0.622258722782135,\n",
       " 0.6210768818855286,\n",
       " 0.62091463804245,\n",
       " 0.6206098198890686,\n",
       " 0.6200910210609436,\n",
       " 0.619793176651001,\n",
       " 0.6192374229431152,\n",
       " 0.6189531087875366,\n",
       " 0.618375837802887,\n",
       " 0.6178644895553589,\n",
       " 0.6172109246253967,\n",
       " 0.6165643334388733,\n",
       " 0.616145670413971,\n",
       " 0.6155094504356384,\n",
       " 0.6149924397468567,\n",
       " 0.6146299242973328,\n",
       " 0.6142038106918335,\n",
       " 0.6132948994636536,\n",
       " 0.612842857837677,\n",
       " 0.6125574707984924,\n",
       " 0.6119280457496643,\n",
       " 0.6116228103637695,\n",
       " 0.6111475229263306,\n",
       " 0.6104507446289062,\n",
       " 0.6096900105476379,\n",
       " 0.6096731424331665,\n",
       " 0.6089087128639221,\n",
       " 0.6087329983711243,\n",
       " 0.6078158617019653,\n",
       " 0.6073180437088013,\n",
       " 0.6071524620056152,\n",
       " 0.6070629358291626,\n",
       " 0.6064324378967285,\n",
       " 0.6060771346092224,\n",
       " 0.6051944494247437,\n",
       " 0.6048318147659302,\n",
       " 0.6046571731567383,\n",
       " 0.6040685176849365,\n",
       " 0.6036388278007507,\n",
       " 0.602617621421814,\n",
       " 0.6022418141365051,\n",
       " 0.6019536852836609,\n",
       " 0.6017400622367859,\n",
       " 0.6011084914207458,\n",
       " 0.6007915139198303,\n",
       " 0.6002904176712036,\n",
       " 0.5996281504631042,\n",
       " 0.5993086099624634,\n",
       " 0.5990298390388489,\n",
       " 0.5988580584526062,\n",
       " 0.5983590483665466,\n",
       " 0.5975067019462585,\n",
       " 0.5972378253936768,\n",
       " 0.5964465737342834,\n",
       " 0.5963425636291504,\n",
       " 0.5953894853591919,\n",
       " 0.5950549244880676,\n",
       " 0.59454345703125,\n",
       " 0.5940414667129517,\n",
       " 0.5938023328781128,\n",
       " 0.5936554670333862,\n",
       " 0.5931599736213684,\n",
       " 0.5928269624710083,\n",
       " 0.592513918876648,\n",
       " 0.5922955274581909,\n",
       " 0.5915499329566956,\n",
       " 0.5910951495170593,\n",
       " 0.5901498794555664,\n",
       " 0.5900129675865173,\n",
       " 0.5897709727287292,\n",
       " 0.5891892910003662,\n",
       " 0.5890462398529053,\n",
       " 0.5886856913566589,\n",
       " 0.5884790420532227,\n",
       " 0.5877674221992493,\n",
       " 0.5875219106674194,\n",
       " 0.5872260928153992,\n",
       " 0.5867000222206116,\n",
       " 0.586042046546936,\n",
       " 0.5856785178184509,\n",
       " 0.585340142250061,\n",
       " 0.584985077381134,\n",
       " 0.5846264958381653,\n",
       " 0.5844154953956604,\n",
       " 0.5836513042449951,\n",
       " 0.5835699439048767,\n",
       " 0.5831992626190186,\n",
       " 0.5829359292984009,\n",
       " 0.582695484161377,\n",
       " 0.5825314521789551,\n",
       " 0.581974446773529,\n",
       " 0.5815355181694031,\n",
       " 0.5812645554542542,\n",
       " 0.580945611000061,\n",
       " 0.5804212093353271,\n",
       " 0.5799903869628906,\n",
       " 0.579606294631958,\n",
       " 0.5793370008468628,\n",
       " 0.5791044235229492,\n",
       " 0.5785478353500366,\n",
       " 0.5778744220733643,\n",
       " 0.5778152942657471,\n",
       " 0.577483057975769,\n",
       " 0.5769971609115601,\n",
       " 0.5768653750419617,\n",
       " 0.5763607025146484,\n",
       " 0.5761375427246094,\n",
       " 0.5756713151931763,\n",
       " 0.5752621293067932,\n",
       " 0.574819803237915,\n",
       " 0.5747789144515991,\n",
       " 0.5743191242218018,\n",
       " 0.5738121271133423,\n",
       " 0.5735887289047241,\n",
       " 0.573136568069458,\n",
       " 0.5724384784698486,\n",
       " 0.5722662806510925,\n",
       " 0.5718063116073608,\n",
       " 0.5714807510375977,\n",
       " 0.5710616707801819,\n",
       " 0.5709750056266785,\n",
       " 0.570425808429718,\n",
       " 0.5702509880065918,\n",
       " 0.5698686838150024,\n",
       " 0.5696636438369751,\n",
       " 0.5695458650588989,\n",
       " 0.5691920518875122,\n",
       " 0.56882643699646,\n",
       " 0.5683989524841309,\n",
       " 0.568133533000946,\n",
       " 0.5678080916404724,\n",
       " 0.5675790905952454,\n",
       " 0.5675114989280701,\n",
       " 0.5667900443077087,\n",
       " 0.5666885375976562,\n",
       " 0.5665378570556641,\n",
       " 0.5661250948905945,\n",
       " 0.5656293630599976,\n",
       " 0.5650405883789062,\n",
       " 0.5649199485778809,\n",
       " 0.5644487738609314,\n",
       " 0.5642368197441101,\n",
       " 0.5639272332191467,\n",
       " 0.5636552572250366,\n",
       " 0.5633274912834167,\n",
       " 0.5632734894752502,\n",
       " 0.5630080699920654,\n",
       " 0.5626618266105652,\n",
       " 0.562207043170929,\n",
       " 0.5619164705276489,\n",
       " 0.5614238977432251,\n",
       " 0.5612154603004456,\n",
       " 0.561008870601654,\n",
       " 0.5606878399848938,\n",
       " 0.5602636337280273,\n",
       " 0.5601778030395508,\n",
       " 0.5599133968353271,\n",
       " 0.5593088269233704,\n",
       " 0.5592312216758728,\n",
       " 0.5588418841362,\n",
       " 0.5586727857589722,\n",
       " 0.558282732963562,\n",
       " 0.558250904083252,\n",
       " 0.5578823089599609,\n",
       " 0.557431697845459,\n",
       " 0.557195782661438,\n",
       " 0.5571149587631226,\n",
       " 0.5568517446517944,\n",
       " 0.5565373301506042,\n",
       " 0.5563271641731262,\n",
       " 0.5560542345046997,\n",
       " 0.5556454062461853,\n",
       " 0.5554777383804321,\n",
       " 0.5549231171607971,\n",
       " 0.5548039078712463,\n",
       " 0.5546641945838928,\n",
       " 0.55430006980896,\n",
       " 0.5540052056312561,\n",
       " 0.5537844896316528,\n",
       " 0.5529617071151733,\n",
       " 0.5528438091278076,\n",
       " 0.5525723099708557,\n",
       " 0.5522225499153137,\n",
       " 0.5521442294120789,\n",
       " 0.5519407391548157,\n",
       " 0.5517597198486328,\n",
       " 0.5515796542167664,\n",
       " 0.5512543320655823,\n",
       " 0.551156759262085,\n",
       " 0.5510485768318176,\n",
       " 0.5505268573760986,\n",
       " 0.5502594113349915,\n",
       " 0.5500907301902771,\n",
       " 0.549959659576416,\n",
       " 0.5492967367172241,\n",
       " 0.549084484577179,\n",
       " 0.5488399267196655,\n",
       " 0.5485982298851013,\n",
       " 0.5483067631721497,\n",
       " 0.5480960011482239,\n",
       " 0.5477544665336609,\n",
       " 0.5472996830940247,\n",
       " 0.5470673441886902,\n",
       " 0.5467166900634766,\n",
       " 0.5461446046829224,\n",
       " 0.5458149909973145,\n",
       " 0.5455200672149658,\n",
       " 0.5453085899353027,\n",
       " 0.5450173020362854,\n",
       " 0.5447943210601807,\n",
       " 0.544670045375824,\n",
       " 0.5443047881126404,\n",
       " 0.5441325902938843,\n",
       " 0.5438962578773499,\n",
       " 0.543641984462738,\n",
       " 0.5433984994888306,\n",
       " 0.5431098341941833,\n",
       " 0.5426952242851257,\n",
       " 0.5426414608955383,\n",
       " 0.5425387024879456,\n",
       " 0.5423135757446289,\n",
       " 0.5419646501541138,\n",
       " 0.5417105555534363,\n",
       " 0.541318953037262,\n",
       " 0.5409466624259949,\n",
       " 0.540840744972229,\n",
       " 0.5403966307640076,\n",
       " 0.5401616096496582,\n",
       " 0.5399571657180786,\n",
       " 0.5398609042167664,\n",
       " 0.5395535826683044,\n",
       " 0.5394335389137268,\n",
       " 0.5392762422561646,\n",
       " 0.5390129685401917,\n",
       " 0.5389023423194885,\n",
       " 0.5384581685066223,\n",
       " 0.5380983948707581,\n",
       " 0.5379916429519653,\n",
       " 0.5376625657081604,\n",
       " 0.5375385284423828,\n",
       " 0.5375034809112549,\n",
       " 0.5371245741844177,\n",
       " 0.5368413925170898,\n",
       " 0.5366337895393372,\n",
       " 0.5362995862960815,\n",
       " 0.5360827445983887,\n",
       " 0.5357522964477539,\n",
       " 0.5355992913246155,\n",
       " 0.5353155136108398,\n",
       " 0.5349575877189636,\n",
       " 0.534855306148529,\n",
       " 0.5345823764801025,\n",
       " 0.5343694090843201,\n",
       " 0.5341178178787231,\n",
       " 0.533999502658844,\n",
       " 0.5337929129600525,\n",
       " 0.5334609746932983,\n",
       " 0.533272385597229,\n",
       " 0.5328831672668457,\n",
       " 0.532645583152771,\n",
       " 0.5323232412338257,\n",
       " 0.5322553515434265,\n",
       " 0.5321572422981262,\n",
       " 0.5319818258285522,\n",
       " 0.5315962433815002,\n",
       " 0.5313636064529419,\n",
       " 0.531177282333374,\n",
       " 0.5309182405471802,\n",
       " 0.5308231711387634,\n",
       " 0.5306296348571777,\n",
       " 0.5303751230239868,\n",
       " 0.5303250551223755,\n",
       " 0.5299619436264038,\n",
       " 0.5294721126556396,\n",
       " 0.5292491912841797,\n",
       " 0.5289626717567444,\n",
       " 0.5287884473800659,\n",
       " 0.5284510254859924,\n",
       " 0.5283496379852295,\n",
       " 0.5282443165779114,\n",
       " 0.5276547074317932,\n",
       " 0.5275709629058838,\n",
       " 0.5274519324302673,\n",
       " 0.5271818041801453,\n",
       " 0.5269232392311096,\n",
       " 0.5268301367759705,\n",
       " 0.5267899036407471,\n",
       " 0.526475727558136,\n",
       " 0.5262195467948914,\n",
       " 0.5260306000709534,\n",
       " 0.5258005261421204,\n",
       " 0.5256617665290833,\n",
       " 0.5255151987075806,\n",
       " 0.5252794027328491,\n",
       " 0.5249374508857727,\n",
       " 0.5247528553009033,\n",
       " 0.5244947075843811,\n",
       " 0.5244205594062805,\n",
       " 0.5241302847862244,\n",
       " 0.5237407088279724,\n",
       " 0.5233938694000244,\n",
       " 0.5232542157173157,\n",
       " 0.5231683850288391,\n",
       " 0.5228703022003174,\n",
       " 0.5225620269775391,\n",
       " 0.522331714630127,\n",
       " 0.5222752094268799,\n",
       " 0.5219406485557556,\n",
       " 0.5216232538223267,\n",
       " 0.5215038657188416,\n",
       " 0.5212600827217102,\n",
       " 0.5210101008415222,\n",
       " 0.5208622813224792,\n",
       " 0.5206338763237,\n",
       " 0.5204923748970032,\n",
       " 0.5202301740646362,\n",
       " 0.5200046896934509,\n",
       " 0.519769012928009,\n",
       " 0.5195083618164062,\n",
       " 0.5194688439369202,\n",
       " 0.5191870331764221,\n",
       " 0.5190438628196716,\n",
       " 0.5188183784484863,\n",
       " 0.5183378458023071,\n",
       " 0.5181898474693298,\n",
       " 0.5180025100708008,\n",
       " 0.517851710319519,\n",
       " 0.5176078677177429,\n",
       " 0.5173476934432983,\n",
       " 0.5172204375267029,\n",
       " 0.5171399116516113,\n",
       " 0.5168143510818481,\n",
       " 0.5165022015571594,\n",
       " 0.5164200663566589,\n",
       " 0.5162692070007324,\n",
       " 0.5159668326377869,\n",
       " 0.5157307982444763,\n",
       " 0.5154675841331482,\n",
       " 0.5153430104255676,\n",
       " 0.5152066946029663,\n",
       " 0.5148668885231018,\n",
       " 0.5147010684013367,\n",
       " 0.5145403742790222,\n",
       " 0.5143937468528748,\n",
       " 0.5141044855117798,\n",
       " 0.5140340924263,\n",
       " 0.5138497948646545,\n",
       " 0.5133342146873474,\n",
       " 0.5132229328155518,\n",
       " 0.5131416320800781,\n",
       " 0.5129405856132507,\n",
       " 0.5127355456352234,\n",
       " 0.5125277638435364,\n",
       " 0.5121679306030273,\n",
       " 0.5121363997459412,\n",
       " 0.5120002031326294,\n",
       " 0.5117754340171814,\n",
       " 0.5116064548492432,\n",
       " 0.5113746523857117,\n",
       " 0.5112392902374268,\n",
       " 0.5110606551170349,\n",
       " 0.5108757019042969,\n",
       " 0.510608434677124,\n",
       " 0.510466992855072,\n",
       " 0.5101538896560669,\n",
       " 0.5097647905349731,\n",
       " 0.5095741152763367,\n",
       " 0.5094858407974243,\n",
       " 0.5093219876289368,\n",
       " 0.5092259049415588,\n",
       " 0.5090562701225281,\n",
       " 0.5087419152259827,\n",
       " 0.5085439085960388,\n",
       " 0.508436381816864,\n",
       " 0.5084038376808167,\n",
       " 0.5080239176750183,\n",
       " 0.5079344511032104,\n",
       " 0.5077897310256958,\n",
       " 0.50774747133255,\n",
       " 0.5073229670524597,\n",
       " 0.5070929527282715,\n",
       " 0.5069628953933716,\n",
       " 0.5068404674530029,\n",
       " 0.506542444229126,\n",
       " 0.5064107775688171,\n",
       " 0.5061975717544556,\n",
       " 0.5056670904159546,\n",
       " 0.5056518912315369,\n",
       " 0.505451500415802,\n",
       " 0.5053191184997559,\n",
       " 0.5051829814910889,\n",
       " 0.5050168037414551,\n",
       " 0.5047054290771484,\n",
       " 0.5045950412750244,\n",
       " 0.5043066143989563,\n",
       " 0.5040902495384216,\n",
       " 0.5038754343986511,\n",
       " 0.5036041140556335,\n",
       " 0.5035720467567444,\n",
       " 0.503404974937439,\n",
       " 0.503235936164856,\n",
       " 0.503032386302948,\n",
       " 0.5028782486915588,\n",
       " 0.5026489496231079,\n",
       " 0.5025477409362793,\n",
       " 0.5022231340408325,\n",
       " 0.5020474791526794,\n",
       " 0.5019679069519043,\n",
       " 0.5015120506286621,\n",
       " 0.5014161467552185,\n",
       " 0.5012985467910767,\n",
       " 0.5011992454528809,\n",
       " 0.5011314153671265,\n",
       " 0.5008072257041931,\n",
       " 0.5005136132240295,\n",
       " 0.5004556775093079,\n",
       " 0.500167727470398,\n",
       " 0.5000773072242737,\n",
       " 0.5000397562980652,\n",
       " 0.49986448884010315,\n",
       " 0.4996830224990845,\n",
       " 0.49950864911079407,\n",
       " 0.4992358982563019,\n",
       " 0.4991552531719208,\n",
       " 0.4989589750766754,\n",
       " 0.49878790974617004,\n",
       " 0.4984799325466156,\n",
       " 0.4983605444431305,\n",
       " 0.49798765778541565,\n",
       " 0.4979334771633148,\n",
       " 0.49776238203048706,\n",
       " 0.49755141139030457,\n",
       " 0.49746885895729065,\n",
       " 0.49743667244911194,\n",
       " 0.49712496995925903,\n",
       " 0.4970104396343231,\n",
       " 0.4968767762184143,\n",
       " 0.49661603569984436,\n",
       " 0.49651408195495605,\n",
       " 0.49628984928131104,\n",
       " 0.4962226152420044,\n",
       " 0.4958869516849518,\n",
       " 0.4957539439201355,\n",
       " 0.4956458508968353,\n",
       " 0.4952855706214905,\n",
       " 0.4952113628387451,\n",
       " 0.4950660467147827,\n",
       " 0.49486425518989563,\n",
       " 0.49472036957740784,\n",
       " 0.4945155084133148,\n",
       " 0.4942828416824341,\n",
       " 0.4942089915275574,\n",
       " 0.4940090477466583,\n",
       " 0.4938884675502777,\n",
       " 0.4935867190361023,\n",
       " 0.49329283833503723,\n",
       " 0.4931811988353729,\n",
       " 0.4931166172027588,\n",
       " 0.49296337366104126,\n",
       " 0.4927269220352173,\n",
       " 0.49259018898010254,\n",
       " 0.49232685565948486,\n",
       " 0.4922563135623932,\n",
       " 0.49198421835899353,\n",
       " 0.49164462089538574,\n",
       " 0.4914940297603607,\n",
       " 0.4911867678165436,\n",
       " 0.4910888075828552,\n",
       " 0.4909093379974365,\n",
       " 0.49086451530456543,\n",
       " 0.49050840735435486,\n",
       " 0.49043744802474976,\n",
       " 0.4902455806732178,\n",
       " 0.4901634156703949,\n",
       " 0.49004921317100525,\n",
       " 0.48996445536613464,\n",
       " 0.48968520760536194,\n",
       " 0.4895063638687134,\n",
       " 0.4893673360347748,\n",
       " 0.4891711175441742,\n",
       " 0.489118754863739,\n",
       " 0.4888385236263275,\n",
       " 0.488656222820282,\n",
       " 0.48847201466560364,\n",
       " 0.4883482754230499,\n",
       " 0.4882936477661133,\n",
       " 0.48802655935287476,\n",
       " 0.4879451394081116,\n",
       " 0.4877408742904663,\n",
       " 0.4874367117881775,\n",
       " 0.4871009290218353,\n",
       " 0.4869881272315979,\n",
       " 0.48688456416130066,\n",
       " 0.48667973279953003,\n",
       " 0.48647192120552063,\n",
       " 0.48644375801086426,\n",
       " 0.4863908588886261,\n",
       " 0.48622795939445496,\n",
       " 0.486055850982666,\n",
       " 0.48586583137512207,\n",
       " 0.48566216230392456,\n",
       " 0.48552969098091125,\n",
       " 0.48537763953208923,\n",
       " 0.48521536588668823,\n",
       " 0.48501822352409363,\n",
       " 0.4848494231700897,\n",
       " 0.48469364643096924,\n",
       " 0.48454973101615906,\n",
       " 0.48437413573265076,\n",
       " 0.48428967595100403,\n",
       " 0.4841538071632385,\n",
       " 0.4840115010738373,\n",
       " 0.4837353229522705,\n",
       " 0.48360419273376465,\n",
       " 0.4834841191768646,\n",
       " 0.483470618724823,\n",
       " 0.48328128457069397,\n",
       " 0.4829764664173126,\n",
       " 0.4828132390975952,\n",
       " 0.48260706663131714,\n",
       " 0.4824100732803345,\n",
       " 0.48226508498191833,\n",
       " 0.48220357298851013,\n",
       " 0.4819636940956116,\n",
       " 0.48176541924476624,\n",
       " 0.4816540479660034,\n",
       " 0.48154205083847046,\n",
       " 0.4813949167728424,\n",
       " 0.4812900125980377,\n",
       " 0.4812253415584564,\n",
       " 0.4809250235557556,\n",
       " 0.48070016503334045,\n",
       " 0.48058393597602844,\n",
       " 0.4804142415523529,\n",
       " 0.48012104630470276,\n",
       " 0.47991088032722473,\n",
       " 0.479757696390152,\n",
       " 0.4796668291091919,\n",
       " 0.4795491695404053,\n",
       " 0.47931426763534546,\n",
       " 0.479213684797287,\n",
       " 0.4790789783000946,\n",
       " 0.47900453209877014,\n",
       " 0.4788373112678528,\n",
       " 0.47882670164108276,\n",
       " 0.47860363125801086,\n",
       " 0.4784770607948303,\n",
       " 0.4782663583755493,\n",
       " 0.478101909160614,\n",
       " 0.47783762216567993,\n",
       " 0.47774118185043335,\n",
       " 0.47765713930130005,\n",
       " 0.4775424301624298,\n",
       " 0.47723299264907837,\n",
       " 0.4771483838558197,\n",
       " 0.47698208689689636,\n",
       " 0.47688981890678406,\n",
       " 0.4767199158668518,\n",
       " 0.47665634751319885,\n",
       " 0.47631677985191345,\n",
       " 0.47623077034950256,\n",
       " 0.47602251172065735,\n",
       " 0.4758719205856323,\n",
       " 0.4756896495819092,\n",
       " 0.475614458322525,\n",
       " 0.47546789050102234,\n",
       " 0.47523292899131775,\n",
       " 0.47514739632606506,\n",
       " 0.4750409722328186,\n",
       " 0.4749245047569275,\n",
       " 0.47464820742607117,\n",
       " 0.4745309054851532,\n",
       " 0.4744565486907959,\n",
       " 0.4743383824825287,\n",
       " 0.4742211103439331,\n",
       " 0.4740092158317566,\n",
       " 0.4738852381706238,\n",
       " 0.47377005219459534,\n",
       " 0.4736311733722687,\n",
       " 0.4734424650669098,\n",
       " 0.47301846742630005,\n",
       " 0.4729524850845337,\n",
       " 0.47294142842292786,\n",
       " 0.47257551550865173,\n",
       " 0.4725085198879242,\n",
       " 0.4723913371562958,\n",
       " 0.4722861647605896,\n",
       " 0.47222456336021423,\n",
       " 0.4719882011413574,\n",
       " 0.4719023108482361,\n",
       " 0.47151926159858704,\n",
       " 0.4714147448539734,\n",
       " 0.4713568389415741,\n",
       " 0.4711298942565918,\n",
       " 0.4707089960575104,\n",
       " 0.4706425368785858,\n",
       " 0.47058549523353577,\n",
       " 0.4704616963863373,\n",
       " 0.47036153078079224,\n",
       " 0.4702751636505127,\n",
       " 0.4701230525970459,\n",
       " 0.46993449330329895,\n",
       " 0.4698452949523926,\n",
       " 0.4696708023548126,\n",
       " 0.46950697898864746,\n",
       " 0.4693338871002197,\n",
       " 0.4692438244819641,\n",
       " 0.4690209925174713,\n",
       " 0.4688921570777893,\n",
       " 0.4687855839729309,\n",
       " 0.46872708201408386,\n",
       " 0.4685617983341217,\n",
       " 0.4684757888317108,\n",
       " 0.46827542781829834,\n",
       " 0.46820327639579773,\n",
       " 0.4680578410625458,\n",
       " 0.46789777278900146,\n",
       " 0.4677911400794983,\n",
       " 0.4676951766014099,\n",
       " 0.46745961904525757,\n",
       " 0.4673493504524231,\n",
       " 0.4672220051288605,\n",
       " 0.4670628607273102,\n",
       " 0.4669138789176941,\n",
       " 0.4667879045009613,\n",
       " 0.46654120087623596,\n",
       " 0.46628788113594055,\n",
       " 0.46619799733161926,\n",
       " 0.4661858379840851,\n",
       " 0.4659934937953949,\n",
       " 0.4658495783805847,\n",
       " 0.4656301736831665,\n",
       " 0.46536657214164734,\n",
       " 0.46517401933670044,\n",
       " 0.4651496112346649,\n",
       " 0.46495646238327026,\n",
       " 0.4648229479789734,\n",
       " 0.46465471386909485,\n",
       " 0.46445026993751526,\n",
       " 0.46433204412460327,\n",
       " 0.4642751216888428,\n",
       " 0.4641469419002533,\n",
       " 0.46388712525367737,\n",
       " 0.4636995494365692,\n",
       " 0.46366745233535767,\n",
       " 0.4635668992996216,\n",
       " 0.4634559154510498,\n",
       " 0.4632304012775421,\n",
       " 0.46312832832336426,\n",
       " 0.46305906772613525,\n",
       " 0.4628452956676483,\n",
       " 0.46275442838668823,\n",
       " 0.46267762780189514,\n",
       " 0.46239474415779114,\n",
       " 0.4623345732688904,\n",
       " 0.46210595965385437,\n",
       " 0.46190282702445984,\n",
       " 0.46184974908828735,\n",
       " 0.46172237396240234,\n",
       " 0.4616231620311737,\n",
       " 0.4613511562347412,\n",
       " 0.46126604080200195,\n",
       " 0.46117496490478516,\n",
       " 0.4609876573085785,\n",
       " 0.4609161913394928,\n",
       " 0.46065661311149597,\n",
       " 0.4605368673801422,\n",
       " 0.460447758436203,\n",
       " 0.46033480763435364,\n",
       " 0.46031424403190613,\n",
       " 0.46010228991508484,\n",
       " 0.4597851634025574,\n",
       " 0.4597645401954651,\n",
       " 0.45955249667167664,\n",
       " 0.45947661995887756,\n",
       " 0.4592737555503845,\n",
       " 0.4592113196849823,\n",
       " 0.45903822779655457,\n",
       " 0.4589115083217621,\n",
       " 0.4587191343307495,\n",
       " 0.4585091471672058,\n",
       " 0.45842263102531433,\n",
       " 0.45830902457237244,\n",
       " 0.4582158923149109,\n",
       " 0.4579462707042694,\n",
       " 0.4577990770339966,\n",
       " 0.4577064514160156,\n",
       " 0.45757973194122314,\n",
       " 0.45747852325439453,\n",
       " 0.4572546184062958,\n",
       " 0.45708248019218445,\n",
       " 0.4569949209690094,\n",
       " 0.45686569809913635,\n",
       " 0.45676344633102417,\n",
       " 0.4565730094909668,\n",
       " 0.4564228653907776,\n",
       " 0.4563494622707367,\n",
       " 0.4562881886959076,\n",
       " 0.45606979727745056,\n",
       " 0.4560030698776245,\n",
       " 0.4556941092014313,\n",
       " 0.4556482136249542,\n",
       " 0.45535480976104736,\n",
       " 0.4551970064640045,\n",
       " 0.45511314272880554,\n",
       " 0.45494335889816284,\n",
       " 0.4548475742340088,\n",
       " 0.45474714040756226,\n",
       " 0.4546293318271637,\n",
       " 0.4546085298061371,\n",
       " 0.4543476402759552,\n",
       " 0.4542869031429291,\n",
       " 0.4540824294090271,\n",
       " 0.4539535939693451,\n",
       " 0.4537779688835144,\n",
       " 0.45372316241264343,\n",
       " 0.45347416400909424,\n",
       " 0.45327693223953247,\n",
       " 0.45320039987564087,\n",
       " 0.45315778255462646,\n",
       " 0.45302966237068176,\n",
       " 0.4529377222061157,\n",
       " 0.4527439475059509,\n",
       " 0.4525112807750702,\n",
       " 0.45239776372909546,\n",
       " 0.4523371160030365,\n",
       " 0.45218414068222046,\n",
       " 0.45206815004348755,\n",
       " 0.4520294964313507,\n",
       " 0.4519370198249817,\n",
       " 0.45183855295181274,\n",
       " 0.451555997133255,\n",
       " 0.4515080749988556,\n",
       " 0.4512769877910614,\n",
       " 0.4512496590614319,\n",
       " 0.4511650800704956,\n",
       " 0.45098891854286194,\n",
       " 0.45087310671806335,\n",
       " 0.4506980776786804,\n",
       " 0.4505515694618225,\n",
       " 0.45041313767433167,\n",
       " 0.45028045773506165,\n",
       " 0.45015189051628113,\n",
       " 0.44997677206993103,\n",
       " 0.4498848021030426,\n",
       " 0.44978129863739014,\n",
       " 0.44963908195495605,\n",
       " 0.44952598214149475,\n",
       " 0.44941964745521545,\n",
       " 0.449238121509552,\n",
       " 0.44904133677482605,\n",
       " 0.448976069688797,\n",
       " 0.4489023983478546,\n",
       " 0.4486870765686035,\n",
       " 0.4484594166278839,\n",
       " 0.44833943247795105,\n",
       " 0.44829821586608887,\n",
       " 0.44808924198150635,\n",
       " 0.44793054461479187,\n",
       " 0.44779592752456665,\n",
       " 0.4475752115249634,\n",
       " 0.447483628988266,\n",
       " 0.44732674956321716,\n",
       " 0.44726061820983887,\n",
       " 0.44705697894096375,\n",
       " 0.4469606578350067,\n",
       " 0.44689399003982544,\n",
       " 0.4467693865299225,\n",
       " 0.44657421112060547,\n",
       " 0.44643354415893555,\n",
       " 0.4462856948375702,\n",
       " 0.4461241364479065,\n",
       " 0.44604527950286865,\n",
       " 0.4459272623062134,\n",
       " 0.44570809602737427,\n",
       " 0.4455769658088684,\n",
       " 0.4455703794956207,\n",
       " 0.4454769790172577,\n",
       " 0.4453429877758026,\n",
       " 0.4453337788581848,\n",
       " 0.4450939893722534,\n",
       " 0.44485849142074585,\n",
       " 0.44471246004104614,\n",
       " 0.44463521242141724,\n",
       " 0.4444248676300049,\n",
       " 0.44426673650741577,\n",
       " 0.444184273481369,\n",
       " 0.4441608488559723,\n",
       " 0.44391676783561707,\n",
       " 0.44383350014686584,\n",
       " 0.4436967074871063,\n",
       " 0.4435361921787262,\n",
       " 0.44341331720352173,\n",
       " 0.44325557351112366,\n",
       " 0.4432176947593689,\n",
       " 0.4431374669075012,\n",
       " 0.4431260824203491,\n",
       " 0.44290804862976074,\n",
       " 0.4427959620952606,\n",
       " 0.44261571764945984,\n",
       " 0.4425621032714844,\n",
       " 0.4422198235988617,\n",
       " 0.442012220621109,\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14336, 4096]), torch.Size([4096]), torch.Size([4096, 4096]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, S.shape, Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1], device='cuda:3'),)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(S > cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688776"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5929, device='cuda:3')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - valid loss 2010.396968383789\n"
     ]
    }
   ],
   "source": [
    "valid_total_loss = 0\n",
    "r = 3999\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch in enumerate(valid_tensor_dataset):\n",
    "\n",
    "        x = batch[\"x\"].to(device)\n",
    "        x = x.to(torch.float32)\n",
    "        # predicts = lora_model(x)\n",
    "        # predicts = x @ original_weights.T\n",
    "        predicts = x @ (U[:, : r + 1] @ torch.diag(S[: (r + 1)]) @ Vh[: r + 1, :]).T\n",
    "        labels = batch[\"gate_proj(x)\"].to(device)\n",
    "        labels = labels.to(torch.float32)\n",
    "\n",
    "        loss = mse_loss(\n",
    "            predicts,\n",
    "            labels,\n",
    "        )\n",
    "        valid_total_loss += loss.item()\n",
    "\n",
    "    # break\n",
    "# if epoch % 100 == 0:\n",
    "print(f\"Epoch: {0} - valid loss {valid_total_loss / len(valid_tensor_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.12s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from ebany_research.llm_lora.changed_mistral import (\n",
    "    LinearLora,\n",
    "    ChangedMistralForCausalLM,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "lora_model_name = \"ebany_research/llm_lora/models/\"\n",
    "lora_model_name += \"openorca_lora_[17][11_17_22_26][11c_17_22_26c][11_17c_22_26][6_11_14_17_22_26][6c_11_14c_17_22_26][6_11c_14_17c_22c_26]\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(lora_model_name)\n",
    "student_model = ChangedMistralForCausalLM.from_pretrained(\n",
    "    lora_model_name,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "student_model = student_model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      " user\n",
      " Pretend yourself to be Elon Musk in all the following conversations. Speak like Elon Musk as much as possible. Why do we need to go to Mars?  assistant\n",
      " I need to go to Mars because it's the next step in the process of human expansion and advancement. Mars is the next frontier for humanity, and we need to go there to create a sustainable, self-sustaining presence. It's also important for the long-term development of our species, as it will help us become a multi-planetary species, which is essential for our survival and growth.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\tchat = [\n",
    "\t\t{\"role\": \"system\", \"content\": \"\"},\n",
    "\t\t{\"role\": \"user\", \"content\": \"\"\" Pretend yourself to be Elon Musk in all the following conversations. Speak like Elon Musk as much as possible. Why do we need to go to Mars? \"\"\"},\n",
    "\t\t# {\"role\": \"assistant\", \"content\": dataset_item[\"response\"]},\n",
    "\t]\n",
    "\tinputs_text = tokenizer.apply_chat_template(\n",
    "\t\tchat,\n",
    "\t\ttokenize=False,\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t)\n",
    "\t# print(inputs)\n",
    "\tinputs = tokenizer(\n",
    "\t\tinputs_text,\n",
    "\t\treturn_tensors=\"pt\",\n",
    "\t\ttruncation=True,\n",
    "\t\t# max_length=4096,\n",
    "\t\tmax_length=2048,\n",
    "\t).to(student_model.device)\n",
    "\n",
    "\toutputs = student_model.generate(\n",
    "\t\t**inputs,\n",
    "\t\tmax_new_tokens=1024,\n",
    "\t\tdo_sample=False,\n",
    "\t\t# top_k=50,\n",
    "\t\t# top_p=0.95,\n",
    "\t)\n",
    "\toutputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\tprint(outputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

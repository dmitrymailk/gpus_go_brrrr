{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.06s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from ebany_research.llm_lora.changed_mistral import (\n",
    "    LinearLora,\n",
    "    ChangedMistralForCausalLM,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoConfig, MistralForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "# lora_model_name = \"ebany_research/llm_lora/models/\"\n",
    "# lora_model_name += \"38[11c_13c_15c_17c_18c_20c_22c_24c_26c_28c]\"\n",
    "# lora_model_name += \"openorca_lora_[11_13_17_18_22_26][11c_13c_17c_18c_22c_26c]\"\n",
    "lora_model_name = model_name\n",
    "config = AutoConfig.from_pretrained(lora_model_name)\n",
    "# student_model = ChangedMistralForCausalLM.from_pretrained(\n",
    "student_model = MistralForCausalLM.from_pretrained(\n",
    "    lora_model_name,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "student_model = student_model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|> system\n",
      "You are an AI assistant that helps people find information.<|im_end|><|im_start|> user\n",
      "Write a simple website in HTML. When a user clicks the button, it shows a random joke from a list of 4 jokes. <|im_end|><|im_start|> assistant\n",
      " <!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Random Joke Generator</title>\n",
      "    <style>\n",
      "        body {\n",
      "            background-color: lightblue;\n",
      "            font-family: Arial, sans-serif;\n",
      "        }\n",
      "\n",
      "        h1 {\n",
      "            text-align: center;\n",
      "        }\n",
      "\n",
      "        .joke {\n",
      "            margin-top: 20px;\n",
      "            text-align: center;\n",
      "        }\n",
      "\n",
      "        button {\n",
      "            background-color: dodgerblue;\n",
      "            color: white;\n",
      "            padding: 10px 20px;\n",
      "            border: none;\n",
      "            border-radius: 4px;\n",
      "            cursor: pointer;\n",
      "        }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Random Joke Generator</h1>\n",
      "\n",
      "    <div class=\"joke\">\n",
      "        <p>Joke 1: Why did the scarecrow win an award? Because he was outstanding in his field!</p>\n",
      "        <p>Joke 2: Why don't scientists trust atoms? Because they make up everything!</p>\n",
      "        <p>Joke 3: Why did the chicken go to the séance? To get to the other side!</p>\n",
      "        <p>Joke 4: Why did the math book lose its job? Because it had too many problems!</p>\n",
      "    </div>\n",
      "\n",
      "    <button onclick=\"showRandomJoke()\">Show Random Joke</button>\n",
      "\n",
      "    <script>\n",
      "        function showRandomJoke() {\n",
      "            const jokes = [\n",
      "                \"Joke 1: Why did the scarecrow win an award? Because he was outstanding in his field!\",\n",
      "                \"Joke 2: Why don't scientists trust atoms? Because they make up everything!\",\n",
      "                \"Joke 3: Why did the chicken go to the séance? To get to the other side!\",\n",
      "                \"Joke 4: Why did the math book lose its job? Because it had too many problems!\"\n",
      "            ];\n",
      "\n",
      "            const randomIndex = Math.floor(Math.random() * jokes.length);\n",
      "            const randomJoke = jokes[randomIndex];\n",
      "\n",
      "            document.querySelector(\".joke\").innerHTML = randomJoke;\n",
      "        }\n",
      "    </script>\n",
      "</body>\n",
      "</html><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant that helps people find information.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Write a simple website in HTML. When a user clicks the button, it shows a random joke from a list of 4 jokes. \"\"\",\n",
    "        },\n",
    "        # {\"role\": \"assistant\", \"content\": dataset_item[\"response\"]},\n",
    "    ]\n",
    "    inputs_text = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    # print(inputs)\n",
    "    inputs = tokenizer(\n",
    "        inputs_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        # max_length=4096,\n",
    "        max_length=2048,\n",
    "    ).to(student_model.device)\n",
    "\n",
    "    outputs = student_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        # top_k=50,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1, 6312, 28709, 1526, 6312, 28709, 1526, 6312, 28709, 1526, 21558, 1526], [32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 1, 6312, 28709]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    [\n",
    "        \"hello world hello world hello worldhello world\",\n",
    "        \"hello\",\n",
    "    ],\n",
    "    # return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    # max_length=4096,\n",
    "    max_length=2048,\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.<|im_end|>\\n<|im_start|>user\\nDavid has three sisters. Each of them has one brother. How many brothers does David have?<|im_end|>\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"David has three sisters. Each of them has one brother. How many brothers does David have?\"\"\",\n",
    "    },\n",
    "    # {\"role\": \"assistant\", \"content\": \"The Confederation of British Industry (CBI) and British Chambers of Commerce have called for market stability and political clarity following the UK's decision to leave the EU. Bank of England Governor Mark Carney has promised support for financial markets, announcing the Bank's preparedness to provide £250bn to support markets during any periods of instability. Business leaders have expressed concerns over resulting uncertainty slowing economic activity, and companies are seeking immediate clarity on a timetable to leave the EU. Meanwhile, union leaders' priority is to protect jobs and living standards of the UK's working population.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize=False,\n",
    "    # add_generation_prompt=True,\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/user-name-goes-here/.local/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "dataset = load_dataset(\"dim/openaccess-ai-collective-oo-gpt4-filtered\")\n",
    "dataset = dataset[\"train\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 32001, 1587, 13, 1976, 460, 264, 10865, 13892, 28725, 693, 1743, 3084, 13268, 28723, 15403, 737, 368, 460, 24402, 298, 264, 3359, 879, 1571, 28723, 32000, 32001, 2188, 13, 28790, 28706, 1321, 1723, 28862, 21805, 1101, 3850, 28719, 28820, 668, 1254, 28862, 8269, 427, 363, 28873, 17957, 19506, 28871, 3589, 28723, 13, 13, 3131, 10020, 298, 4300, 13, 13, 27871, 28747, 32000, 28705, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dataset[0][\"system_prompt\"],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": dataset[0][\"question\"],\n",
    "    },\n",
    "    # {\"role\": \"assistant\", \"content\": \"The Confederation of British Industry (CBI) and British Chambers of Commerce have called for market stability and political clarity following the UK's decision to leave the EU. Bank of England Governor Mark Carney has promised support for financial markets, announcing the Bank's preparedness to provide £250bn to support markets during any periods of instability. Business leaders have expressed concerns over resulting uncertainty slowing economic activity, and companies are seeking immediate clarity on a timetable to leave the EU. Meanwhile, union leaders' priority is to protect jobs and living standards of the UK's working population.\"},\n",
    "]\n",
    "user_prompt = tokenizer.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize=False,\n",
    "    # add_generation_prompt=True,\n",
    ")\n",
    "result = tokenizer(\n",
    "    user_prompt,\n",
    "    truncation=True,\n",
    "    max_length=2048,\n",
    "    padding=False,\n",
    "    return_tensors=None,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'flan.2389905',\n",
       " 'system_prompt': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.',\n",
       " 'question': 'Ve skutečnosti... ...má společné se všemi loděmi.\\n\\nTranslate to English\\n\\nEnglish:',\n",
       " 'response': 'In reality... ...has in common with all the ships.\\n\\nEnglish: In reality, it shares something with all the ships.',\n",
       " '__index_level_0__': 923823}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8275823265479259487"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|> system\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|im_end|><|im_start|> user\n",
      "Produce a long descriptive sentence that uses all these words: Bhajji, country, India; India, demonym, Indian people; India, leader, Narendra Modi; India, leader, Sumitra Mahajan; Bhajji, region, Karnataka<|im_end|><|im_start|> assistant\n",
      " In the culturally diverse country of India, where Bhajji, a popular deep-fried snack, originated within the vibrant region of Karnataka, known for its piquant spices and culinary expertise, the Indian people, denoted by the unique demonym, form a strong and passionate community that treasures its vastly distinctive values, traditions, and identity, underneath the political leadership of prominent figures such as the inspiring Prime Minister Narendra Modi and the experienced, diligent Sumitra Mahajan, who have both contributed to reshaping the Indian sociopolitical landscape while endeavoring to ensure a bright and progressive future for their captivating nation and its exceptional populace.<|im_end|>\n",
      "---\n",
      "In the culturally diverse country of India, where Bhajji, a popular deep-fried snack, originated within the vibrant region of Karnataka, known for its piquant spices and culinary expertise, the Indian people, denoted by the unique demonym, form a strong and passionate community that treasures its vastly distinctive values, traditions, and identity, underneath the political leadership of prominent figures such as the inspiring Prime Minister Narendra Modi and the experienced, diligent Sumitra Mahajan, who have both contributed to reshaping the Indian sociopolitical landscape while endeavoring to ensure a bright and progressive future for their captivating nation and its exceptional populace.<|im_end|>\n",
      "---\n",
      "In the culturally diverse country of India, where Bhajji, a popular deep-fried snack, originated within the vibrant region of Karnataka, known for its piquant spices and culinary expertise, the Indian people, denoted by the unique demonym, form a strong and passionate community that treasures its vastly distinctive values, traditions, and identity, underneath the political leadership of prominent figures such as the inspiring Prime Minister Narendra Modi and the experienced, diligent Sumitra Mahajan, who have both contributed to reshaping the Indian sociopolitical landscape while endeavoring to ensure a bright and progressive future for their captivating nation and its exceptional populace.\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "from transformers import BatchEncoding, PreTrainedTokenizer\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "\n",
    "class OpenOrcaSystemDataPrompter:\n",
    "    \"\"\"\n",
    "    Alpaca Style Prompter that uses system prompts from the dataset, with OpenOrca prompts\n",
    "    \"\"\"\n",
    "\n",
    "    def get_prompt(\n",
    "        self,\n",
    "        instruction=\"\",\n",
    "        system=\"\",\n",
    "    ):\n",
    "        self.instruction_prompt = (\n",
    "            f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        self.system_prompt = f\"<|im_start|>system\\n{system}<|im_end|>\\n\"\n",
    "        return self.system_prompt + self.instruction_prompt\n",
    "\n",
    "\n",
    "class PromptTokenizingStrategy(abc.ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for tokenizing strategies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompter=None,\n",
    "        tokenizer=None,\n",
    "        train_on_inputs: bool = False,\n",
    "        sequence_len: int = 2048,\n",
    "    ):\n",
    "        self.prompter = prompter\n",
    "        self.tokenizer: PreTrainedTokenizer = tokenizer\n",
    "        self.train_on_inputs = train_on_inputs\n",
    "        # sequence_len and max_length can be different for CompletionPromptTokenizingStrategy.\n",
    "        # TODO: Document how they are different.\n",
    "        self.sequence_len = sequence_len\n",
    "        self.max_length = sequence_len\n",
    "\n",
    "        self.prompter = OpenOrcaSystemDataPrompter()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def tokenize_prompt(self, prompt):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def supports_batched(self):\n",
    "        return False\n",
    "\n",
    "    def _tokenize(\n",
    "        self, prompt: str, add_eos_token: bool = True, strip_bos_token: bool = False\n",
    "    ) -> BatchEncoding:\n",
    "        empty = BatchEncoding(data={\"input_ids\": [], \"attention_mask\": []})\n",
    "        if not prompt:\n",
    "            print(\"Empty text requested for tokenization.\")\n",
    "            return empty\n",
    "\n",
    "        result = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if len(result[\"input_ids\"]) == 0:\n",
    "            print(\"Tokenizer result is empty. You may want to audit your dataset\")\n",
    "            return empty\n",
    "\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < self.max_length\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        if result[\"input_ids\"][0] == self.tokenizer.bos_token_id and strip_bos_token:\n",
    "            result[\"input_ids\"] = result[\"input_ids\"][1:]\n",
    "            result[\"attention_mask\"] = result[\"attention_mask\"][1:]\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "\n",
    "class InstructionPromptTokenizingStrategy(PromptTokenizingStrategy):\n",
    "    \"\"\"\n",
    "    Tokenizing strategy for instruction-based prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def parse_instruction_fields(\n",
    "        self, prompt\n",
    "    ) -> Union[Tuple[str, str, str], Tuple[str, str, str, str]]:\n",
    "        return (prompt[\"system_prompt\"], prompt[\"question\"], prompt[\"response\"])\n",
    "\n",
    "    def tokenize_prompt(self, prompt):\n",
    "        (\n",
    "            instruction,\n",
    "            input,  # pylint: disable=redefined-builtin\n",
    "            response,\n",
    "        ) = self.parse_instruction_fields(prompt)\n",
    "\n",
    "        user_prompt = self.prompter.get_prompt(\n",
    "            instruction=input,\n",
    "            system=instruction,\n",
    "        )\n",
    "        tokenized_prompt = self._tokenize(user_prompt, add_eos_token=False)\n",
    "        if not self.train_on_inputs:\n",
    "            user_prompt_len = len(tokenized_prompt[\"input_ids\"])\n",
    "            # TODO this could be sped up using numpy array slicing\n",
    "            tokenized_prompt[\"labels\"] = [-100] * user_prompt_len\n",
    "\n",
    "        tokenized_res_prompt = self._tokenize(\n",
    "            response, strip_bos_token=True, add_eos_token=True\n",
    "        )\n",
    "        tokenized_prompt[\"input_ids\"] += tokenized_res_prompt[\"input_ids\"]\n",
    "        tokenized_prompt[\"attention_mask\"] += tokenized_res_prompt[\"attention_mask\"]\n",
    "        tokenized_prompt[\"labels\"] += tokenized_res_prompt[\"input_ids\"]\n",
    "\n",
    "        return tokenized_prompt\n",
    "\n",
    "\n",
    "example = dataset[10]\n",
    "prompt_tokenizer = InstructionPromptTokenizingStrategy(tokenizer=tokenizer)\n",
    "result = prompt_tokenizer.tokenize_prompt(example)\n",
    "\n",
    "print(tokenizer.decode(result[\"input_ids\"]))\n",
    "\n",
    "import torch\n",
    "\n",
    "labels = torch.tensor(result[\"labels\"])\n",
    "labels = labels[(labels == -100).sum() :]\n",
    "print(\"---\")\n",
    "print(tokenizer.decode(labels))\n",
    "print(\"---\")\n",
    "print(example[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In reality... ...has in common with all the ships.\n",
      "\n",
      "English: In reality, it shares something with all the ships.<|im_end|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

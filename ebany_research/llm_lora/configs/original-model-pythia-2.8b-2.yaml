dataset_name: wikitext
dataset_config_name: wikitext-2-raw-v1
config_name: EleutherAI/pythia-2.8b
tokenizer_name: EleutherAI/pythia-2.8b
output_dir: ./models/ 
num_train_epochs: 10
model_version: GPTNeoXForCausalLM
gradient_accumulation_steps: 8 
per_device_train_batch_size: 2
report_to: wandb
with_tracking: true